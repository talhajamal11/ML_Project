{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jandsy/ml_finance_imperial/blob/main/Coursework/CourseWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSIsUTeyXNr_"
      },
      "source": [
        "# **<center>Machine Learning and Finance </center>**\n",
        "\n",
        "\n",
        "## <center> CourseWork 2024 - StatArb </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBDj0uD44dQL"
      },
      "source": [
        "\n",
        "In this coursework, you will delve into and replicate selected elements of the research detailed in the paper **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**. **However, we will not reproduce the entire study.**\n",
        "\n",
        "## Overview\n",
        "\n",
        "This study redefines Statistical Arbitrage (StatArb) by combining Autoencoder architectures and policy learning to generate trading strategies. Traditionally, StatArb involves finding the mean of a synthetic asset through classical or PCA-based methods before developing a mean reversion strategy. However, this paper proposes a data-driven approach using an Autoencoder trained on US stock returns, integrated into a neural network representing portfolio trading policies to output portfolio allocations directly.\n",
        "\n",
        "\n",
        "## Coursework Goal\n",
        "\n",
        "This coursework will replicate these results, providing hands-on experience in implementing and evaluating this innovative end-to-end policy learning Autoencoder within financial trading strategies.\n",
        "\n",
        "## Outline\n",
        "\n",
        "- [Data Preparation and Exploration](#Data-Preparation-and-Exploration)\n",
        "- [Fama French Analysis](#Fama-French-Analysis)\n",
        "- [PCA Analysis](#PCA-Analysis)\n",
        "- [Ornstein Uhlenbeck](#Ornstein-Uhlenbeck)\n",
        "- [Autoencoder Analysis](#Autoencoder-Analysis)\n",
        "\n",
        "\n",
        "\n",
        "**Description:**\n",
        "The Coursework is graded on a 100 point scale and is divided into five  parts. Below is the mark distribution for each question:\n",
        "\n",
        "| **Problem**  | **Question**          | **Number of Marks** |\n",
        "|--------------|-----------------------|---------------------|\n",
        "| **Part A**   | Question 1            | 4                   |\n",
        "|              | Question 2            | 1                   |\n",
        "|              | Question 3            | 3                   |\n",
        "|              | Question 4            | 3                   |\n",
        "|              | Question 5            | 1                   |\n",
        "|              | Question 6            | 3                   |\n",
        "|**Part  B**    | Question 7           | 1                   |\n",
        "|              | Question 8            | 5                   |\n",
        "|              | Question 9            | 4                   |\n",
        "|              | Question 10           | 5                   |\n",
        "|              | Question 11           | 2                   |\n",
        "|              | Question 12           | 3                   |\n",
        "|**Part  C**    | Question 13          | 3                   |\n",
        "|              | Question 14           | 1                   |\n",
        "|              | Question 15           | 3                   |\n",
        "|              | Question 16           | 2                   |\n",
        "|              | Question 17           | 7                   |\n",
        "|              | Question 18           | 6                   |\n",
        "|              | Question 19           | 3                   |\n",
        "|  **Part  D** | Question 20           | 3                   |\n",
        "|              | Question 21           | 5                   |\n",
        "|              | Question 22           | 2                   |\n",
        "|  **Part  E** | Question 23           | 2                   |\n",
        "|              | Question 24           | 1                   |\n",
        "|              | Question 25           | 3                   |\n",
        "|              | Question 26           | 10                  |\n",
        "|              | Question 27           | 1                   |\n",
        "|              | Question 28           | 3                   |\n",
        "|              | Question 29           | 3                   |\n",
        "|              | Question 30           | 7                   |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Please read the questions carefully and do your best. Good luck!\n",
        "\n",
        "## Objectives\n",
        "\n",
        "\n",
        "\n",
        "## 1. Data Preparation and Exploration\n",
        "Collect, clean, and prepare US stock return data for analysis.\n",
        "\n",
        "## 2. Fama French Analysis\n",
        "Utilize Fama French Factors to isolate the idiosyncratic components of stock returns, differentiating them from market-wide effects. This analysis helps in understanding the unique characteristics of individual stocks relative to broader market trends.\n",
        "\n",
        "## 3. PCA Analysis\n",
        "Employ Principal Component Analysis (PCA) to identify hidden structures and reduce dimensionality in the data. This method helps in extracting significant patterns that might be obscured in high-dimensional datasets.\n",
        "\n",
        "## 4. Ornstein-Uhlenbeck Process\n",
        "Analyze mean-reverting behavior in stock prices using the Ornstein-Uhlenbeck process. This stochastic process is useful for modeling and forecasting based on the assumption that prices will revert to a long-term mean.\n",
        "\n",
        "## 5. Building a Basic Autoencoder Model\n",
        "Construct and train a standard Autoencoder to extract residual idiosyncratic risk.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFU9ckGplGDf"
      },
      "source": [
        "# Data Preparation and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiqyU4DQlxsV"
      },
      "source": [
        "\n",
        "---\n",
        "<font color=green>Q1: (4 Marks)</font>\n",
        "<br><font color='green'>\n",
        "Write a Python function that accepts a URL parameter and retrieves the NASDAQ-100 companies and their ticker symbols by scraping the relevant Wikipedia page using **[Requests](https://pypi.org/project/requests/)** and **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**. Your function should return the data as a list of tuples, with each tuple containing the company name and its ticker symbol. Then, call your function with the appropriate Wikipedia page URL and print the data in a 'Company: Ticker' format.\n",
        "\n",
        "</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lcJXF6aSxCGu"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         Name Symbol\n",
            "0                  Adobe Inc.   ADBE\n",
            "1                         ADP    ADP\n",
            "2                      Airbnb   ABNB\n",
            "3     Alphabet Inc. (Class A)  GOOGL\n",
            "4     Alphabet Inc. (Class C)   GOOG\n",
            "..                        ...    ...\n",
            "96   Walgreens Boots Alliance    WBA\n",
            "97     Warner Bros. Discovery    WBD\n",
            "98              Workday, Inc.   WDAY\n",
            "99                Xcel Energy    XEL\n",
            "100                   Zscaler     ZS\n",
            "\n",
            "[101 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "def get_nasdaq100_companies(url):\n",
        "    '''\n",
        "    This function takes the url of the wikipedia page of the NASDAQ-100 index\n",
        "    and returns a list of tuples with the name and symbol of the companies in the index.\n",
        "\n",
        "    '''\n",
        "    response = requests.get(url) \n",
        "    soup = BeautifulSoup(response.content, 'html.parser') \n",
        "    table = soup.find('table', {'id': 'constituents'}) \n",
        "    rows = table.find_all('tr') \n",
        "    data = [] \n",
        "    for row in rows[1:]: # we iterate over the rows, skipping the first one\n",
        "        cols = row.find_all('td') \n",
        "        data.append((cols[0].text.strip(), cols[1].text.strip()))\n",
        "    return data\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/NASDAQ-100' # url of the wikipedia page\n",
        "data = get_nasdaq100_companies(url)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Name', 'Symbol']) # we create a DataFrame to make it easier to work with the data\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvyNAF9sE7u"
      },
      "source": [
        "---\n",
        "<font color=green>Q2: (1 Mark)</font>\n",
        "<br><font color='green'>\n",
        "Given a list of tuples representing NASDAQ-100 companies (where each tuple contains a company name and its ticker symbol), write a Python script to extract all ticker symbols into a separate list called `tickers_list`.\n",
        "</font>\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yQaWckkXxDLP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ADBE', 'ADP', 'ABNB', 'GOOGL', 'GOOG', 'AMZN', 'AMD', 'AEP', 'AMGN', 'ADI', 'ANSS', 'AAPL', 'AMAT', 'ASML', 'AZN', 'TEAM', 'ADSK', 'BKR', 'BIIB', 'BKNG', 'AVGO', 'CDNS', 'CDW', 'CHTR', 'CTAS', 'CSCO', 'CCEP', 'CTSH', 'CMCSA', 'CEG', 'CPRT', 'CSGP', 'COST', 'CRWD', 'CSX', 'DDOG', 'DXCM', 'FANG', 'DLTR', 'DASH', 'EA', 'EXC', 'FAST', 'FTNT', 'GEHC', 'GILD', 'GFS', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG', 'KDP', 'KLAC', 'KHC', 'LRCX', 'LIN', 'LULU', 'MAR', 'MRVL', 'MELI', 'META', 'MCHP', 'MU', 'MSFT', 'MRNA', 'MDLZ', 'MDB', 'MNST', 'NFLX', 'NVDA', 'NXPI', 'ORLY', 'ODFL', 'ON', 'PCAR', 'PANW', 'PAYX', 'PYPL', 'PDD', 'PEP', 'QCOM', 'REGN', 'ROP', 'ROST', 'SIRI', 'SBUX', 'SNPS', 'TTWO', 'TMUS', 'TSLA', 'TXN', 'TTD', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZS']\n"
          ]
        }
      ],
      "source": [
        "tickers_list = df['Symbol'].tolist() # get the tickers list\n",
        "print(tickers_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KEkAPUxsW4s"
      },
      "source": [
        "---\n",
        "<font color=green>Q3: (3 Marks)</font>\n",
        "<br><font color='green'>\n",
        "Using **[yfinance](https://pypi.org/project/yfinance/)** library, write a Python script that accepts a list of stock ticker symbols. For each symbol, download the adjusted closing price data, store it in a dictionary with the ticker symbol as the key, and then convert the final dictionary into a Pandas DataFrame. Handle any errors encountered during data retrieval by printing a message indicating which symbol failed\n",
        "</font>\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pynohwbpxEg5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 ADBE         ADP        ABNB       GOOGL  \\\n",
            "Date                                                                        \n",
            "2004-06-07 00:00:00-04:00   23.657122   22.350809         NaN         NaN   \n",
            "2004-06-08 00:00:00-04:00   23.537233   22.286243         NaN         NaN   \n",
            "2004-06-09 00:00:00-04:00   22.952799   22.211487         NaN         NaN   \n",
            "2004-06-10 00:00:00-04:00   22.902847   22.266304         NaN         NaN   \n",
            "2004-06-14 00:00:00-04:00   22.618120   21.922438         NaN         NaN   \n",
            "...                               ...         ...         ...         ...   \n",
            "2024-05-31 00:00:00-04:00  444.760010  244.919998  144.929993  172.500000   \n",
            "2024-06-03 00:00:00-04:00  439.019989  244.020004  146.250000  173.169998   \n",
            "2024-06-04 00:00:00-04:00  448.369995  245.669998  147.080002  173.789993   \n",
            "2024-06-05 00:00:00-04:00  455.799988  245.779999  145.779999  175.410004   \n",
            "2024-06-06 00:00:00-04:00  459.230011  247.804993  146.910004  176.770004   \n",
            "\n",
            "                                 GOOG        AMZN         AMD        AEP  \\\n",
            "Date                                                                       \n",
            "2004-06-07 00:00:00-04:00         NaN    2.588000   15.800000  14.331814   \n",
            "2004-06-08 00:00:00-04:00         NaN    2.597000   16.180000  14.187236   \n",
            "2004-06-09 00:00:00-04:00         NaN    2.512000   15.510000  13.988438   \n",
            "2004-06-10 00:00:00-04:00         NaN    2.497000   15.750000  14.096873   \n",
            "2004-06-14 00:00:00-04:00         NaN    2.462500   15.150000  13.997467   \n",
            "...                               ...         ...         ...        ...   \n",
            "2024-05-31 00:00:00-04:00  173.960007  176.440002  166.899994  90.250000   \n",
            "2024-06-03 00:00:00-04:00  174.419998  178.339996  163.550003  90.080002   \n",
            "2024-06-04 00:00:00-04:00  175.130005  179.339996  159.990005  90.379997   \n",
            "2024-06-05 00:00:00-04:00  177.070007  181.279999  166.169998  88.949997   \n",
            "2024-06-06 00:00:00-04:00  178.225006  183.589996  168.050003  89.040001   \n",
            "\n",
            "                                 AMGN         ADI  ...        TSLA  \\\n",
            "Date                                               ...               \n",
            "2004-06-07 00:00:00-04:00   39.808876   31.409290  ...         NaN   \n",
            "2004-06-08 00:00:00-04:00   39.758896   31.339983  ...         NaN   \n",
            "2004-06-09 00:00:00-04:00   39.373444   30.388765  ...         NaN   \n",
            "2004-06-10 00:00:00-04:00   39.523365   30.489561  ...         NaN   \n",
            "2004-06-14 00:00:00-04:00   39.587589   30.269072  ...         NaN   \n",
            "...                               ...         ...  ...         ...   \n",
            "2024-05-31 00:00:00-04:00  305.850006  233.560974  ...  178.080002   \n",
            "2024-06-03 00:00:00-04:00  307.420013  231.290009  ...  176.289993   \n",
            "2024-06-04 00:00:00-04:00  307.369995  230.630005  ...  174.770004   \n",
            "2024-06-05 00:00:00-04:00  307.380005  235.679993  ...  175.000000   \n",
            "2024-06-06 00:00:00-04:00  306.753387  237.220001  ...  178.100006   \n",
            "\n",
            "                                  TXN        TTD        VRSK        VRTX  \\\n",
            "Date                                                                       \n",
            "2004-06-07 00:00:00-04:00   17.041819        NaN         NaN    9.190000   \n",
            "2004-06-08 00:00:00-04:00   16.918371        NaN         NaN    9.220000   \n",
            "2004-06-09 00:00:00-04:00   16.385609        NaN         NaN    8.980000   \n",
            "2004-06-10 00:00:00-04:00   16.327137        NaN         NaN    9.250000   \n",
            "2004-06-14 00:00:00-04:00   15.891834        NaN         NaN    9.480000   \n",
            "...                               ...        ...         ...         ...   \n",
            "2024-05-31 00:00:00-04:00  195.009995  92.779999  252.779999  455.339996   \n",
            "2024-06-03 00:00:00-04:00  193.720001  93.110001  253.750000  470.179993   \n",
            "2024-06-04 00:00:00-04:00  193.300003  94.480003  258.250000  474.950012   \n",
            "2024-06-05 00:00:00-04:00  196.080002  97.410004  261.279999  483.040009   \n",
            "2024-06-06 00:00:00-04:00  195.779999  97.150002  259.540009  483.540009   \n",
            "\n",
            "                                 WBA     WBD        WDAY        XEL  \\\n",
            "Date                                                                  \n",
            "2004-06-07 00:00:00-04:00  21.202646     NaN         NaN   7.971287   \n",
            "2004-06-08 00:00:00-04:00  21.342890     NaN         NaN   7.900705   \n",
            "2004-06-09 00:00:00-04:00  21.129469     NaN         NaN   7.858355   \n",
            "2004-06-10 00:00:00-04:00  21.111172     NaN         NaN   7.877178   \n",
            "2004-06-14 00:00:00-04:00  21.172155     NaN         NaN   7.825416   \n",
            "...                              ...     ...         ...        ...   \n",
            "2024-05-31 00:00:00-04:00  16.219999  8.2400  211.449997  55.450001   \n",
            "2024-06-03 00:00:00-04:00  15.920000  8.3300  210.830002  55.279999   \n",
            "2024-06-04 00:00:00-04:00  16.110001  8.2400  211.119995  56.029999   \n",
            "2024-06-05 00:00:00-04:00  15.940000  8.3000  212.460007  55.160000   \n",
            "2024-06-06 00:00:00-04:00  15.825000  8.4001  215.100006  54.820000   \n",
            "\n",
            "                                   ZS  \n",
            "Date                                   \n",
            "2004-06-07 00:00:00-04:00         NaN  \n",
            "2004-06-08 00:00:00-04:00         NaN  \n",
            "2004-06-09 00:00:00-04:00         NaN  \n",
            "2004-06-10 00:00:00-04:00         NaN  \n",
            "2004-06-14 00:00:00-04:00         NaN  \n",
            "...                               ...  \n",
            "2024-05-31 00:00:00-04:00  169.960007  \n",
            "2024-06-03 00:00:00-04:00  169.020004  \n",
            "2024-06-04 00:00:00-04:00  169.139999  \n",
            "2024-06-05 00:00:00-04:00  174.570007  \n",
            "2024-06-06 00:00:00-04:00  179.580002  \n",
            "\n",
            "[5035 rows x 101 columns]\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf # import the yfinance library to get the stock data\n",
        "\n",
        "def get_stock_data(tickers_list):\n",
        "    '''\n",
        "    This function gets the stock data for a list of tickers\n",
        "\n",
        "    Parameters:\n",
        "    tickers_list (list): list of stock tickers\n",
        "\n",
        "    Returns:\n",
        "    stock_data (dict): dictionary with the stock data\n",
        "    '''\n",
        "    stock_data = {}\n",
        "    for ticker in tickers_list:\n",
        "        try:\n",
        "            stock = yf.Ticker(ticker)\n",
        "            stock_data[ticker] = stock.history('20y')['Close'] # we chose to get the closing price for the last 20 years\n",
        "        except:\n",
        "            print(f'Failed to get data for {ticker}')\n",
        "    return stock_data\n",
        "\n",
        "stock_data = get_stock_data(tickers_list)\n",
        "df = pd.DataFrame(stock_data) # we create a pandas dataframe with the stock data to make it easier to work with\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np3XHMtatQC_"
      },
      "source": [
        "---\n",
        "<font color=green>Q4: (3 Marks)</font>\n",
        "<br><font color='green'>\n",
        "Write a Python script to analyze stock data stored in a dictionary `stock_data` (where each key is a stock ticker symbol, and each value is a Pandas Series of adjusted closing prices). The script should:\n",
        "1. Convert the dictionary into a DataFrame.\n",
        "2. Calculate the daily returns for each stock.\n",
        "3. Identify columns (ticker symbols) with at least 2000 non-NaN values in their daily returns.\n",
        "4. Create a new DataFrame that only includes these filtered ticker symbols.\n",
        "5. Remove any remaining rows with NaN values in this new DataFrame.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2pt3y9_IxFjz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               ADBE       ADP     GOOGL      GOOG      AMZN  \\\n",
            "Date                                                                          \n",
            "2015-12-10 00:00:00-05:00 -0.006699  0.006004 -0.003292 -0.002861 -0.003715   \n",
            "2015-12-11 00:00:00-05:00  0.027653 -0.024924 -0.012657 -0.014130 -0.033473   \n",
            "2015-12-14 00:00:00-05:00  0.020127  0.015240  0.016151  0.012045  0.027744   \n",
            "2015-12-15 00:00:00-05:00  0.008149  0.010284 -0.003213 -0.005844  0.001110   \n",
            "2015-12-16 00:00:00-05:00  0.016380  0.008541  0.021708  0.019761  0.026008   \n",
            "...                             ...       ...       ...       ...       ...   \n",
            "2024-05-31 00:00:00-04:00 -0.002489  0.016645  0.002266  0.002305 -0.016061   \n",
            "2024-06-03 00:00:00-04:00 -0.012906 -0.003675  0.003884  0.002644  0.010768   \n",
            "2024-06-04 00:00:00-04:00  0.021297  0.006762  0.003580  0.004071  0.005607   \n",
            "2024-06-05 00:00:00-04:00  0.016571  0.000448  0.009322  0.011077  0.010817   \n",
            "2024-06-06 00:00:00-04:00  0.007525  0.008239  0.007753  0.006523  0.012743   \n",
            "\n",
            "                                AMD       AEP      AMGN       ADI      ANSS  \\\n",
            "Date                                                                          \n",
            "2015-12-10 00:00:00-05:00  0.042553 -0.024356  0.011274  0.008826  0.004968   \n",
            "2015-12-11 00:00:00-05:00 -0.036735 -0.005831 -0.028309 -0.003850 -0.013951   \n",
            "2015-12-14 00:00:00-05:00 -0.008475 -0.000367  0.019078 -0.001932  0.003899   \n",
            "2015-12-15 00:00:00-05:00  0.008547  0.027503  0.028524 -0.004752  0.009544   \n",
            "2015-12-16 00:00:00-05:00  0.076271  0.017309  0.012053  0.017330  0.008354   \n",
            "...                             ...       ...       ...       ...       ...   \n",
            "2024-05-31 00:00:00-04:00  0.000900  0.023707  0.016113  0.019522 -0.007690   \n",
            "2024-06-03 00:00:00-04:00 -0.020072 -0.001884  0.005133 -0.009723 -0.008505   \n",
            "2024-06-04 00:00:00-04:00 -0.021767  0.003330 -0.000163 -0.002854  0.007466   \n",
            "2024-06-05 00:00:00-04:00  0.038627 -0.015822  0.000033  0.021896  0.031441   \n",
            "2024-06-06 00:00:00-04:00  0.011314  0.001012 -0.002039  0.006534 -0.010411   \n",
            "\n",
            "                           ...      TTWO      TMUS      TSLA       TXN  \\\n",
            "Date                       ...                                           \n",
            "2015-12-10 00:00:00-05:00  ... -0.004777  0.007485  0.011358  0.002995   \n",
            "2015-12-11 00:00:00-05:00  ... -0.011575 -0.009356 -0.044259 -0.012647   \n",
            "2015-12-14 00:00:00-05:00  ...  0.005141  0.014444  0.007188  0.000178   \n",
            "2015-12-15 00:00:00-05:00  ...  0.023018  0.044907  0.011483  0.023657   \n",
            "2015-12-16 00:00:00-05:00  ...  0.006389  0.025419  0.060699  0.009035   \n",
            "...                        ...       ...       ...       ...       ...   \n",
            "2024-05-31 00:00:00-04:00  ...  0.003630  0.028874 -0.003971 -0.003424   \n",
            "2024-06-03 00:00:00-04:00  ...  0.020454 -0.010745 -0.010052 -0.006615   \n",
            "2024-06-04 00:00:00-04:00  ...  0.014178  0.027906 -0.008622 -0.002168   \n",
            "2024-06-05 00:00:00-04:00  ...  0.008857  0.013265  0.001316  0.014382   \n",
            "2024-06-06 00:00:00-04:00  ... -0.006928 -0.000721  0.017714 -0.001530   \n",
            "\n",
            "                               VRSK      VRTX       WBA       WBD      WDAY  \\\n",
            "Date                                                                          \n",
            "2015-12-10 00:00:00-05:00 -0.002615  0.010279  0.000960 -0.002109  0.005037   \n",
            "2015-12-11 00:00:00-05:00 -0.015996 -0.034709 -0.020499 -0.035928 -0.057630   \n",
            "2015-12-14 00:00:00-05:00  0.014390 -0.016491  0.010403 -0.033613  0.000253   \n",
            "2015-12-15 00:00:00-05:00  0.013924  0.013656 -0.005450  0.002646 -0.000380   \n",
            "2015-12-16 00:00:00-05:00  0.021117  0.010658  0.031665  0.024887  0.029378   \n",
            "...                             ...       ...       ...       ...       ...   \n",
            "2024-05-31 00:00:00-04:00  0.013674  0.027740  0.053931  0.019802  0.019331   \n",
            "2024-06-03 00:00:00-04:00  0.003837  0.032591 -0.018496  0.010922 -0.002932   \n",
            "2024-06-04 00:00:00-04:00  0.017734  0.010145  0.011935 -0.010804  0.001375   \n",
            "2024-06-05 00:00:00-04:00  0.011733  0.017033 -0.010553  0.007282  0.006347   \n",
            "2024-06-06 00:00:00-04:00 -0.006659  0.001035 -0.007215  0.012060  0.012426   \n",
            "\n",
            "                                XEL  \n",
            "Date                                 \n",
            "2015-12-10 00:00:00-05:00 -0.013889  \n",
            "2015-12-11 00:00:00-05:00  0.004311  \n",
            "2015-12-14 00:00:00-05:00  0.010017  \n",
            "2015-12-15 00:00:00-05:00  0.007934  \n",
            "2015-12-16 00:00:00-05:00  0.023897  \n",
            "...                             ...  \n",
            "2024-05-31 00:00:00-04:00  0.020803  \n",
            "2024-06-03 00:00:00-04:00 -0.003066  \n",
            "2024-06-04 00:00:00-04:00  0.013567  \n",
            "2024-06-05 00:00:00-04:00 -0.015527  \n",
            "2024-06-06 00:00:00-04:00 -0.006164  \n",
            "\n",
            "[2136 rows x 89 columns]\n"
          ]
        }
      ],
      "source": [
        "# 1. Convert the dictionary into a DataFrame.\n",
        "df = pd.DataFrame(stock_data)\n",
        "\n",
        "# 2. Calculate the daily returns for each stock.\n",
        "df_daily_returns = df.pct_change()\n",
        "\n",
        "# 3. Identify columns (ticker symbols) with at least 2000 non-NaN values in their daily returns.\n",
        "valid_columns = df_daily_returns.columns[df_daily_returns.notnull().sum() >= 2000]\n",
        "\n",
        "# 4. Create a new DataFrame that only includes these filtered ticker symbols.\n",
        "df_filtered = df_daily_returns[valid_columns]\n",
        "\n",
        "# 5. Remove any remaining rows with NaN values in this new DataFrame.\n",
        "df_filtered = df_filtered.dropna()\n",
        "\n",
        "print(df_filtered) \n",
        "#df_filtered is the final DataFrame with the filtered ticker symbols and no NaN values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xAawJ8nPbH"
      },
      "source": [
        "---\n",
        "<font color=green>Q5: (1 Mark)</font>\n",
        "<br><font color='green'>\n",
        "Download the dataset named `df_filtered_nasdaq_100` from the GitHub repository of the course.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ko4juu_HxHnT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
            "Date                                                                     \n",
            "2015-12-10 -0.006699  0.006004 -0.003292 -0.002861 -0.003715  0.042553   \n",
            "2015-12-11  0.027653 -0.024924 -0.012657 -0.014130 -0.033473 -0.036735   \n",
            "2015-12-14  0.020127  0.015241  0.016151  0.012045  0.027744 -0.008475   \n",
            "2015-12-15  0.008149  0.010283 -0.003213 -0.005844  0.001110  0.008547   \n",
            "2015-12-16  0.016380  0.008541  0.021708  0.019761  0.026008  0.076271   \n",
            "...              ...       ...       ...       ...       ...       ...   \n",
            "2024-05-06  0.015241  0.003514  0.005142  0.004971  0.013372  0.034396   \n",
            "2024-05-07 -0.002674  0.009805  0.018739  0.018548  0.000318 -0.008666   \n",
            "2024-05-08 -0.008471 -0.008894 -0.010920 -0.010521 -0.004026 -0.005245   \n",
            "2024-05-09 -0.011166  0.009097  0.003424  0.002454  0.007979 -0.008007   \n",
            "2024-05-10 -0.000746  0.006975 -0.007708 -0.007518 -0.010660 -0.003084   \n",
            "\n",
            "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
            "Date                                                ...                       \n",
            "2015-12-10 -0.024356  0.011274  0.008826  0.004968  ... -0.004777  0.007485   \n",
            "2015-12-11 -0.005831 -0.028308 -0.003850 -0.013951  ... -0.011575 -0.009356   \n",
            "2015-12-14 -0.000366  0.019078 -0.001932  0.003899  ...  0.005141  0.014444   \n",
            "2015-12-15  0.027503  0.028525 -0.004752  0.009544  ...  0.023018  0.044907   \n",
            "2015-12-16  0.017309  0.012053  0.017330  0.008354  ...  0.006389  0.025419   \n",
            "...              ...       ...       ...       ...  ...       ...       ...   \n",
            "2024-05-06  0.002370 -0.037939  0.018484  0.006478  ...  0.016863 -0.013548   \n",
            "2024-05-07  0.011936  0.002738  0.001230  0.010728  ... -0.000067 -0.001109   \n",
            "2024-05-08  0.007900  0.023343  0.006337  0.005907  ... -0.015910  0.003946   \n",
            "2024-05-09  0.004085  0.018060 -0.000342  0.000887  ... -0.001987  0.011361   \n",
            "2024-05-10  0.007257 -0.008662  0.011719  0.003056  ...  0.001373 -0.002915   \n",
            "\n",
            "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
            "Date                                                                     \n",
            "2015-12-10  0.011358  0.002996 -0.002616  0.010279  0.000960 -0.002109   \n",
            "2015-12-11 -0.044259 -0.012648 -0.015996 -0.034709 -0.020499 -0.035928   \n",
            "2015-12-14  0.007188  0.000178  0.014390 -0.016491  0.010402 -0.033613   \n",
            "2015-12-15  0.011483  0.023657  0.013923  0.013656 -0.005450  0.002646   \n",
            "2015-12-16  0.060699  0.009036  0.021117  0.010658  0.031664  0.024887   \n",
            "...              ...       ...       ...       ...       ...       ...   \n",
            "2024-05-06  0.019703  0.015427  0.019087  0.003540 -0.030881 -0.001255   \n",
            "2024-05-07 -0.037616  0.012752  0.021252  0.019230  0.005214 -0.023869   \n",
            "2024-05-08 -0.017378  0.007007 -0.009838  0.020915 -0.006916  0.003861   \n",
            "2024-05-09 -0.015739  0.007448  0.001676  0.000406  0.001161  0.030769   \n",
            "2024-05-10 -0.020352  0.009335  0.013593  0.009046 -0.003478  0.013682   \n",
            "\n",
            "                WDAY       XEL  \n",
            "Date                            \n",
            "2015-12-10  0.005037 -0.013889  \n",
            "2015-12-11 -0.057630  0.004311  \n",
            "2015-12-14  0.000253  0.010017  \n",
            "2015-12-15 -0.000380  0.007934  \n",
            "2015-12-16  0.029378  0.023897  \n",
            "...              ...       ...  \n",
            "2024-05-06 -0.022949  0.002028  \n",
            "2024-05-07 -0.001921  0.012141  \n",
            "2024-05-08  0.000802 -0.001636  \n",
            "2024-05-09 -0.014702  0.005644  \n",
            "2024-05-10  0.001545  0.003983  \n",
            "\n",
            "[2118 rows x 89 columns]\n"
          ]
        }
      ],
      "source": [
        "# we use github link to get the data\n",
        "nasdaq100 = 'https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/df_filtered_nasdaq_100.csv'\n",
        "\n",
        "# df_filtered_nasdaq_100 are the daily returns of the NASDAQ-100 index\n",
        "df_filtered_nasdaq_100 = pd.read_csv(nasdaq100, index_col=0)\n",
        "print(df_filtered_nasdaq_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WbCUDabWQoZ"
      },
      "source": [
        "---\n",
        "<font color=green>Q6: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Conduct an in-depth analysis of the `df_filtered_nasdaq_100` dataset from GitHub. Answer the following questions:\n",
        "- Which stock had the best performance over the entire period?\n",
        "- What is the average daily return of 'AAPL'?\n",
        "- What is the worst daily return? Provide the stock name and the date it occurred.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kPfiDWWlxI0Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the stock with the best performance over the entire period is NVDA\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "which stock has the best performance over the entire period?\n",
        "'''\n",
        "# first we calculate the sum of the daily returns for each stock\n",
        "stock_returns = df_filtered_nasdaq_100.sum()\n",
        "# then we get the index of the stock with the highest sum\n",
        "best_stock = stock_returns.idxmax() \n",
        "\n",
        "print(f'the stock with the best performance over the entire period is {best_stock}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code Result indicate that NVDA (NVIDIA Corporation), has the best performance over the entire period from 2015 to 2024. This growth can be attributed to NVIDIA's strategic expansion into key markets such as gaming, data centers, artificial intelligence (AI), and autonomous vehicles. Specially with the AI technology trend worldwide, the company has significantly increased its revenue and profits. NVIDIA is leading innovations in GPU technology, which have become critical for AI research and applications. Additionally, the high demand for high performance computing and graphic processing has enhanced NVIDIA’s market position, which increased stock appreciation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average daily return for AAPL is : 0.0010849409515136207\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "What is the average daily return of 'AAPL'?\n",
        "'''\n",
        "average_daily_return = df_filtered_nasdaq_100['AAPL'].mean()\n",
        "print(f'The average daily return for AAPL is :', average_daily_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average daily return for AAPL, Apple Inc., is 0.001085, which reflect its consistent performance and steady growth over the period (2015-2024). This period has seen Apple continuously innovate and expand its product lineup with successful launches like the iPhone, iPad, and Apple Watch, alongside growing services revenue from the App Store, Apple Music, and iCloud. However, what makes apple apple stock return positive is investors believes on Apple's strong brand loyalty, which makes alot of consumers uses apple as first brand product. Also, Apple has efficient supply chain management, and strategic entry into new markets such as wearables and financial services have contributed to its stable returns and robust financial health."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The worst daily return is -0.4464579394678258 and it occurred on 2020-03-09 for FANG (Diamondback Energy, Inc.)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "What is the worst daily return? Provide the stock name and the date it occurred.\n",
        "\n",
        "'''\n",
        "# first we get the worst daily return\n",
        "worst_daily_return = df_filtered_nasdaq_100.min().min()\n",
        "# then we get the date of the worst daily return\n",
        "worst_daily_return_date = df_filtered_nasdaq_100.stack()[df_filtered_nasdaq_100.stack() == worst_daily_return].index[0] # stack method is used to convert the dataframe into a series and then we get the index of the worst daily return\n",
        "# finally we get the stock name\n",
        "stock = worst_daily_return_date[1]\n",
        "stock_name = yf.Ticker(stock).info['longName']\n",
        "# we print the results\n",
        "print(f'The worst daily return is {worst_daily_return} and it occurred on {worst_daily_return_date[0]} for {stock} ({stock_name})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The worst daily return is -0.4464579394678258, which occurred on 2020-03-09 for FANG (Diamondback Energy, Inc.). This significant drop occurs because of the broader market crash in March 2020. The reason of the market crash was the COVID-19 pandemic lockdowns which caused a sharp decline in oil prices and impacted the energy sector. Diamondback Energy heavily involved in the oil and gas industry, which faced intense pressure as demand plummeted and prices fell, leading to a significant decline in its stock value on that day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcY-V82zXGqc"
      },
      "source": [
        "# Fama French Analysis\n",
        "\n",
        "The Fama-French five-factor model is an extension of the classic three-factor model used in finance to describe stock returns. It is designed to better capture the risk associated with stocks and explain differences in returns. This model includes the following factors:\n",
        "\n",
        "1. **Market Risk (MKT)**: The excess return of the market over the risk-free rate. It captures the overall market's premium.\n",
        "2. **Size (SMB, \"Small Minus Big\")**: The performance of small-cap stocks relative to large-cap stocks.\n",
        "3. **Value (HML, \"High Minus Low\")**: The performance of stocks with high book-to-market values relative to those with low book-to-market values.\n",
        "4. **Profitability (RMW, \"Robust Minus Weak\")**: The difference in returns between companies with robust (high) and weak (low) profitability.\n",
        "5. **Investment (CMA, \"Conservative Minus Aggressive\")**: The difference in returns between companies that invest conservatively and those that invest aggressively.\n",
        "\n",
        "## Additional Factor\n",
        "\n",
        "6. **Momentum (MOM)**: This factor represents the tendency of stocks that have performed well in the past to continue performing well, and the reverse for stocks that have performed poorly.\n",
        "\n",
        "### Mathematical Representation\n",
        "\n",
        "The return of a stock $R_i^t$ at time $t$ can be modeled as follows :\n",
        "\n",
        "$$\n",
        "R_i^t - R_f^t = \\alpha_i^t + \\beta_{i,MKT}^t(R_M^t - R_f^t) + \\beta_{i,SMB}^t \\cdot SMB^t + \\beta_{i,HML}^t \\cdot HML^t + \\beta_{i,RMW}^t \\cdot RMW^t + \\beta_{i,CMA}^t \\cdot CMA^t + \\beta_{i,MOM}^t \\cdot MOM^t + \\epsilon_i^t\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ R_i^t $ is the return of stock $i$ at time $t$\n",
        "- $R_f^t $is the risk-free rate at time $t$\n",
        "- $ R_M^t $ is the market return at time $t$\n",
        "- $\\alpha_i^t $ is the abnormal return or alpha of stock $ i $ at time $t$\n",
        "- $\\beta^t $ coefficients represent the sensitivity of the stock returns to each factor at time $t$\n",
        "- $\\epsilon_i^t $ is the error term or idiosyncratic risk unique to stock $ i $ at time $t$\n",
        "\n",
        "This model is particularly useful for identifying which factors significantly impact stock returns and for constructing a diversified portfolio that is optimized for given risk preferences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtFgmpsKQc8B"
      },
      "source": [
        "---\n",
        "<font color=green>Q7: (1 Mark) </font>\n",
        "<br><font color='green'>\n",
        "Download the `fama_french_dataset` from the course's GitHub account.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iJVPHhTSxKuk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Mkt-RF   SMB   HML   RMW   CMA     RF   Mom\n",
            "1963-07-01   -0.67  0.02 -0.35  0.03  0.13  0.012 -0.21\n",
            "1963-07-02    0.79 -0.28  0.28 -0.08 -0.21  0.012  0.42\n",
            "1963-07-03    0.63 -0.18 -0.10  0.13 -0.25  0.012  0.41\n",
            "1963-07-05    0.40  0.09 -0.28  0.07 -0.30  0.012  0.07\n",
            "1963-07-08   -0.63  0.07 -0.20 -0.27  0.06  0.012 -0.45\n",
            "...            ...   ...   ...   ...   ...    ...   ...\n",
            "2024-03-22   -0.23 -0.98 -0.53  0.29 -0.37  0.021  0.43\n",
            "2024-03-25   -0.26 -0.10  0.88 -0.22 -0.17  0.021 -0.34\n",
            "2024-03-26   -0.26  0.10 -0.13 -0.50  0.23  0.021  0.09\n",
            "2024-03-27    0.88  1.29  0.91 -0.14  0.58  0.021 -1.34\n",
            "2024-03-28    0.10  0.45  0.48 -0.07  0.09  0.021 -0.44\n",
            "\n",
            "[15290 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# we download the Fama-French dataset from github\n",
        "fama_french_dataset = 'https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/fama_french_dataset.csv'\n",
        "df_fama_french = pd.read_csv(fama_french_dataset, index_col=0)\n",
        "print(df_fama_french)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5FJ362KW-wh"
      },
      "source": [
        "---\n",
        "<font color=green>Q8: (5 Marks)</font>\n",
        "<br><font color='green'>\n",
        "\n",
        "Write a Python function called `get_sub_df_ticker(ticker, date, df_filtered, length_history)` that extracts a historical sub-dataframe for a given `ticker` from `df_filtered`. The function should use `length_history` to determine the number of trading days to include, ending at the specified `date`. Return the sub-dataframe for the specified `ticker`.\n",
        "</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sub_df_ticker(ticker, date, df_filtered, length_history):\n",
        "    \"\"\"\n",
        "    Extract a subset of data for a specific ticker within a defined date range.\n",
        "\n",
        "    Parameters:\n",
        "    ticker (str): The ticker symbol to filter.\n",
        "    date (str or pd.Timestamp): The end date for the data subset.\n",
        "    df_filtered (pd.DataFrame): The DataFrame containing the stock data.\n",
        "    length_history (int): The length of the history to look back in years.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The subset of the DataFrame containing the specified ticker's data.\n",
        "    \"\"\"\n",
        "    # Convert the end date to a pandas Timestamp\n",
        "    end_date = pd.to_datetime(date)\n",
        "    \n",
        "    # Ensure the DataFrame index is in datetime format\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_filtered.index):\n",
        "        df_filtered.index = pd.to_datetime(df_filtered.index)\n",
        "\n",
        "    # Calculate the start date by subtracting the length of history from the end date\n",
        "    start_date = end_date - pd.DateOffset(years=length_history)\n",
        "\n",
        "    # Ensure the date range exists within the DataFrame index\n",
        "    if start_date < df_filtered.index.min():\n",
        "        start_date = df_filtered.index.min()\n",
        "    if end_date > df_filtered.index.max():\n",
        "        end_date = df_filtered.index.max()\n",
        "\n",
        "    # Ensure the ticker exists in the DataFrame columns\n",
        "    if ticker not in df_filtered.columns:\n",
        "        raise ValueError(f\"Ticker '{ticker}' not found in the DataFrame columns\")\n",
        "\n",
        "    # Extract the subset of data within the date range for the specified ticker\n",
        "    sub_df = df_filtered.loc[start_date:end_date, [ticker]]\n",
        "\n",
        "    return sub_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                AAPL\n",
            "Date                \n",
            "2016-01-04  0.000855\n",
            "2016-01-05 -0.025059\n",
            "2016-01-06 -0.019570\n",
            "2016-01-07 -0.042205\n",
            "2016-01-08  0.005288\n",
            "...              ...\n",
            "2020-12-24  0.007712\n",
            "2020-12-28  0.035766\n",
            "2020-12-29 -0.013315\n",
            "2020-12-30 -0.008527\n",
            "2020-12-31 -0.007703\n",
            "\n",
            "[1259 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "# test the function using the 'AAPL' ticker\n",
        "sub_df = get_sub_df_ticker('AAPL', '2021-01-01', df_filtered_nasdaq_100, 5)\n",
        "print(sub_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-p7nCNYXYNr"
      },
      "source": [
        "---\n",
        "<font color=green>Q9: (4 Marks)</font>\n",
        "<br><font color='green'>\n",
        "Create a Python function named `df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data)` that uses `get_sub_df_ticker` to extract historical data for a specific `ticker`. Incorporate the Fama-French factors from `fama_french_data` into the extracted sub-dataframe. Adjust the ticker's returns by subtracting the risk-free rate ('RF') and add other relevant Fama-French factors ('Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', and 'Mom'). Return the resulting sub-dataframe.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pfsdj79HxMnp"
      },
      "outputs": [],
      "source": [
        "def df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data):\n",
        "    \"\"\"\n",
        "    Integrate Fama-French data with the stock data for a specific ticker.\n",
        "\n",
        "    Parameters:\n",
        "    ticker (str): The ticker symbol to filter.\n",
        "    date (str or pd.Timestamp): The end date for the data subset.\n",
        "    df_filtered (pd.DataFrame): The DataFrame containing the stock data.\n",
        "    length_history (int): The length of the history to look back in years.\n",
        "    fama_french_data (pd.DataFrame): The DataFrame containing Fama-French factors.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The combined DataFrame with adjusted returns and Fama-French factors.\n",
        "    \"\"\"\n",
        "    # Get the subset of the data for the specified ticker\n",
        "    sub_df = get_sub_df_ticker(ticker, date, df_filtered, length_history)\n",
        "\n",
        "    # Ensure Fama-French data index is in datetime format\n",
        "    if not pd.api.types.is_datetime64_any_dtype(fama_french_data.index):\n",
        "        fama_french_data.index = pd.to_datetime(fama_french_data.index)\n",
        "\n",
        "    # Join the Fama-French data with the stock data\n",
        "    sub_df = sub_df.join(fama_french_data, how='inner')\n",
        "\n",
        "    # Adjusting the returns by subtracting the risk-free rate\n",
        "    sub_df['Adj_Return'] = sub_df[ticker] - sub_df['RF']\n",
        "\n",
        "    # Add other relevant Fama-French factors\n",
        "    factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']\n",
        "    for factor in factors:\n",
        "        if factor in fama_french_data.columns:\n",
        "            sub_df[factor] = sub_df[factor]\n",
        "\n",
        "    return sub_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AAPL</th>\n",
              "      <th>Mkt-RF</th>\n",
              "      <th>SMB</th>\n",
              "      <th>HML</th>\n",
              "      <th>RMW</th>\n",
              "      <th>CMA</th>\n",
              "      <th>RF</th>\n",
              "      <th>Mom</th>\n",
              "      <th>Adj_Return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-04</th>\n",
              "      <td>0.000855</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-0.76</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.93</td>\n",
              "      <td>0.000855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-05</th>\n",
              "      <td>-0.025059</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>-0.025059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-06</th>\n",
              "      <td>-0.019570</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.92</td>\n",
              "      <td>-0.019570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-07</th>\n",
              "      <td>-0.042205</td>\n",
              "      <td>-2.44</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>-0.042205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-08</th>\n",
              "      <td>0.005288</td>\n",
              "      <td>-1.11</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.005288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-24</th>\n",
              "      <td>0.007712</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.007712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-28</th>\n",
              "      <td>0.035766</td>\n",
              "      <td>0.46</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>0.035766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-29</th>\n",
              "      <td>-0.013315</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.78</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>-0.013315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-30</th>\n",
              "      <td>-0.008527</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.008527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-31</th>\n",
              "      <td>-0.007703</td>\n",
              "      <td>0.39</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>-0.007703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1259 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                AAPL  Mkt-RF   SMB   HML   RMW   CMA   RF   Mom  Adj_Return\n",
              "2016-01-04  0.000855   -1.59 -0.76  0.52  0.35  0.42  0.0 -1.93    0.000855\n",
              "2016-01-05 -0.025059    0.12 -0.24  0.01  0.05  0.31  0.0  0.67   -0.025059\n",
              "2016-01-06 -0.019570   -1.35 -0.23  0.00  0.15  0.04  0.0  1.92   -0.019570\n",
              "2016-01-07 -0.042205   -2.44 -0.29  0.08  0.49  0.36  0.0  0.86   -0.042205\n",
              "2016-01-08  0.005288   -1.11 -0.52 -0.03  0.25  0.05  0.0 -0.11    0.005288\n",
              "...              ...     ...   ...   ...   ...   ...  ...   ...         ...\n",
              "2020-12-24  0.007712    0.21 -0.44 -0.19  0.25 -0.07  0.0  0.20    0.007712\n",
              "2020-12-28  0.035766    0.46 -0.68  0.36  1.39  0.46  0.0 -0.47    0.035766\n",
              "2020-12-29 -0.013315   -0.40 -1.42  0.24  0.78 -0.29  0.0 -0.41   -0.013315\n",
              "2020-12-30 -0.008527    0.27  1.06  0.03 -0.65 -0.05  0.0 -0.27   -0.008527\n",
              "2020-12-31 -0.007703    0.39 -0.71  0.41  0.57 -0.24  0.0 -0.59   -0.007703\n",
              "\n",
              "[1259 rows x 9 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test the fama_french function using the 'AAPL' ticker\n",
        "sub_df_fama_french = df_ticker_with_fama_french('AAPL', '2021-01-01', df_filtered_nasdaq_100, 5, df_fama_french)\n",
        "sub_df_fama_french"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVqmW4PQe5T"
      },
      "source": [
        "---\n",
        "<font color=green>Q10: (5 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Write a Python function named `extract_beta_fama_french` to perform a rolling regression analysis for a given stock at a specific time point using the Fama-French model. The function should accept the following parameters:\n",
        "\n",
        "- `ticker`: A string indicating the stock symbol.\n",
        "- `date`: A string specifying the date for the analysis.\n",
        "- `length_history`: An integer representing the number of days of historical data to include.\n",
        "- `df_filtered`: A pandas DataFrame (assumed to be derived from question 5) containing filtered stock data.\n",
        "- `fama_french_data`: A pandas DataFrame (assumed to be from question 7) that includes Fama-French factors.\n",
        "\n",
        "Utilize the `statsmodels.api` library to conduct the regression.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VGSUB3UDxN2B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def extract_beta_fama_french(ticker, date, length_history, df_filtered, fama_french_data):\n",
        "    \"\"\"\n",
        "    Perform a rolling regression analysis for a given stock at a specific time point using the Fama-French model.\n",
        "\n",
        "    Parameters:\n",
        "    ticker (str): The stock symbol.\n",
        "    date (str): The date for the analysis.\n",
        "    length_history (int): The number of days of historical data to include.\n",
        "    df_filtered (pd.DataFrame): The DataFrame containing the filtered stock data.\n",
        "    fama_french_data (pd.DataFrame): The DataFrame containing Fama-French factors.\n",
        "\n",
        "    Returns:\n",
        "    np.array: The array of beta values from the rolling regression analysis.\n",
        "    \"\"\"\n",
        "    # Get the subset of the data for the specified ticker with Fama-French factors\n",
        "    sub_df = df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data)\n",
        "    windowsize = 60\n",
        "    sub_df = sub_df.iloc[windowsize:]\n",
        "\n",
        "    # Define the independent variables (Fama-French factors)\n",
        "    X = sub_df[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']]\n",
        "\n",
        "    # Define the dependent variable (adjusted returns)\n",
        "    y = sub_df['Adj_Return']\n",
        "   \n",
        "    # Initialize an empty list to store the beta values\n",
        "    betas = []\n",
        "   \n",
        "\n",
        "    # Perform the rolling regression analysis\n",
        "    for i in range(windowsize, len(X)):\n",
        "        X_temp = X.iloc[i-windowsize:i]\n",
        "        X_temp = sm.add_constant(X_temp)\n",
        "        y_temp = y.iloc[i-windowsize:i]\n",
        "        model = sm.OLS(y_temp, X_temp)\n",
        "        results = model.fit()\n",
        "        \n",
        "\n",
        "        # Extract the beta value from the regression results\n",
        "        beta = results.params['Mkt-RF']\n",
        "\n",
        "        betas.append(beta)\n",
        "\n",
        "    # Return the last model results object instead of the beta values\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY8FqrBjRDTz"
      },
      "source": [
        "---\n",
        "<font color=green>Q11: (2 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Apply the `extract_beta_fama_french` function to the stock symbol 'AAPL' for the date '2024-03-28', using a historical data length of 252 days. Ensure that the `df_filtered` and `fama_french_data` DataFrames are correctly prepared and available in your environment before executing this function. The parameters for the function call are set as follows:\n",
        "\n",
        "- **Ticker**: 'AAPL'\n",
        "- **Date**: '2024-03-28'\n",
        "- **Length of History**: 252 days\n",
        "</font>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fo6C5-qXxPZO"
      },
      "outputs": [],
      "source": [
        "# Extract the beta using the Fama-French model\n",
        "Ticker = 'AAPL'\n",
        "Date = '2024-03-28'\n",
        "Length_History = 252\n",
        "APPL_beta = extract_beta_fama_french(Ticker, Date, Length_History, df_filtered_nasdaq_100, df_fama_french)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DyA4G1d0HjY"
      },
      "source": [
        "---\n",
        "<font color=green>Q12: (2 Marks)</font>\n",
        "<br><font color='green'>\n",
        "Once the `extract_beta_fama_french` function has been applied to 'AAPL' with the specified parameters, the next step is to analyze the regression summary to identify which Fama-French factor explains the most variance in 'AAPL' returns during the specified period.\n",
        "\n",
        "Follow these steps to perform the analysis:\n",
        "\n",
        "1. **Review the Summary**: Examine the regression output, focusing on the coefficients and their statistical significance (p-values).\n",
        "2. **Identify Key Factor**: Determine which factor has the highest absolute coefficient value and is statistically significant (typically p < 0.05). This factor can be considered as having the strongest influence on 'AAPL' returns for the period.\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:             Adj_Return   R-squared:                       0.385\n",
            "Model:                            OLS   Adj. R-squared:                  0.316\n",
            "Method:                 Least Squares   F-statistic:                     5.535\n",
            "Date:                Thu, 06 Jun 2024   Prob (F-statistic):           0.000164\n",
            "Time:                        20:14:39   Log-Likelihood:                 189.08\n",
            "No. Observations:                  60   AIC:                            -364.2\n",
            "Df Residuals:                      53   BIC:                            -349.5\n",
            "Df Model:                           6                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         -0.0245      0.002    -15.734      0.000      -0.028      -0.021\n",
            "Mkt-RF         0.0109      0.003      3.855      0.000       0.005       0.017\n",
            "SMB            0.0018      0.003      0.700      0.487      -0.003       0.007\n",
            "HML           -0.0084      0.003     -2.773      0.008      -0.014      -0.002\n",
            "RMW            0.0045      0.004      1.246      0.218      -0.003       0.012\n",
            "CMA            0.0101      0.006      1.574      0.122      -0.003       0.023\n",
            "Mom            0.0009      0.003      0.303      0.763      -0.005       0.007\n",
            "==============================================================================\n",
            "Omnibus:                        9.612   Durbin-Watson:                   1.752\n",
            "Prob(Omnibus):                  0.008   Jarque-Bera (JB):               11.505\n",
            "Skew:                          -0.633   Prob(JB):                      0.00317\n",
            "Kurtosis:                       4.732   Cond. No.                         6.17\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "print(APPL_beta.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The key factor with the highest absolute coefficient value and statistical significance for AAPL returns is: const\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "2. Identify Key Factor: Determine which factor has the highest absolute coefficient value and is statistically significant (typically p < 0.05).\n",
        " This factor can be considered as having the strongest influence on 'AAPL' returns for the period.\n",
        "\n",
        "'''\n",
        "# Get the factor with the highest absolute coefficient value\n",
        "key_factor = APPL_beta.params.abs().idxmax()\n",
        "# Check if the factor is statistically significant\n",
        "if APPL_beta.pvalues[key_factor] < 0.05:\n",
        "    print(f'The key factor with the highest absolute coefficient value and statistical significance for AAPL returns is: {key_factor}')\n",
        "else:\n",
        "    print('No statistically significant key factor found')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaIsaiFWWNO"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The rolling regression output provides various metrics and coefficients for the factors affecting adjusted returns. The R-squared value is 0.385, indicating that the model explains approximately 38.5% of the variation in adjusted returns. The Adjusted R-squared is 0.316, which is close to the R-squared value and accounts for the number of predictors in the model. The F-statistic is 5.535 with a p-value of 0.000164, indicating that the model is statistically significant.\n",
        "\n",
        "Here are the coefficients and p-values for each factor:\n",
        "\n",
        "- **Mkt-RF**: 0.0109, p < 0.0001\n",
        "- **SMB**: 0.0018, p = 0.487\n",
        "- **HML**: -0.0084, p = 0.008\n",
        "- **RMW**: 0.0045, p = 0.218\n",
        "- **CMA**: 0.0101, p = 0.122\n",
        "- **Mom**: 0.0009, p = 0.763\n",
        "\n",
        "#### Key Factors:\n",
        "We focus on the absolute values of the coefficients and their significance levels to determine which factors have the greatest impact within our dataset. This approach helps us understand the influence these factors have on economic variables, such as interest rates, inflation, or other relevant variables considered in analytical studies spanning several years across various industries globally. Key determinants driving performance across sectors over the last decade can be identified by examining these factors. Those with small p-values (<5%) include:\n",
        "\n",
        "- **Mkt-RF**: 0.0109\n",
        "- **HML**: -0.0084\n",
        "\n",
        "Among these, the factors with the highest absolute coefficient values are **Mkt-RF** and **HML** with coefficients of 0.0109 and -0.0084, respectively. This suggests that both factors significantly explain the variability in adjusted returns, with **Mkt-RF** having a slightly greater impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj7qlAq-J8N2"
      },
      "source": [
        "# PCA Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qty6V05YXxti"
      },
      "source": [
        "\n",
        "In literature, another method exists for extracting residuals for each stock, utilizing the PCA approach to identify hidden factors in the data. Let's describe this method.\n",
        "\n",
        "The return of a stock $R_i^t$ at time $t$ can be modeled as follows :\n",
        "\n",
        "$$\n",
        "R_i^t  = \\sum_{j=1}^m\\beta_{i,j}^t F_j^t  + \\epsilon_i^t\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ R_i^t $ is the return of stock $i$ at time $t$\n",
        "- $m$ is the number of factors selected from PCA\n",
        "-  $ F_j^t $ is the $j$-th hidden factor constructed from PCA at time $t$\n",
        "- $\\beta_{i,j}^t $ are the coefficients representing the sensitivity of the stock returns to each hidden factor.\n",
        "- $\\epsilon_i^t $  is the residual term for stock $i$ at time $t$, representing the portion of the return not explained by the PCA factors.\n",
        "\n",
        "### Representation of Stock Return Data\n",
        "\n",
        "Consider the return data for $N$ stocks over $T$ periods, represented by the matrix $R$ of size $T \\times N$:\n",
        "\n",
        "$$\n",
        "R = \\left[\n",
        "\\begin{array}{cccc}\n",
        "R_1^T & R_2^T & \\cdots & R_N^T \\\\\n",
        "R_1^{T-1} & R_2^{T-1} & \\cdots & R_N^{T-1} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "R_1^1 & R_2^1 & \\cdots & R_N^1 \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Each element $R_i^k$ of the matrix represents the return of stock $i$ at time $k$ and is defined as:\n",
        "\n",
        "$$\n",
        "R_i^k = \\frac{S_{i,k} - S_{i, k-1}}{S_{i, k-1}}, \\quad k=1,\\cdots, T, \\quad i=1,\\cdots,N\n",
        "$$\n",
        "\n",
        "where $S_{i,k}$ denotes the adjusted close price of stock $i$ at time $k$.\n",
        "\n",
        "### Standardization of Returns\n",
        "\n",
        "To adjust for varying volatilities across stocks, we standardize the returns as follows:\n",
        "\n",
        "$$\n",
        "Z_i^t = \\frac{R_i^t - \\mu_i}{\\sigma_i}\n",
        "$$\n",
        "\n",
        "where $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of returns for stock $i$ over the period $[t-T, t]$, respectively.\n",
        "\n",
        "### Empirical Correlation Matrix\n",
        "\n",
        "The empirical correlation matrix $C$ is computed from the standardized returns:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{T-1} Z^T Z\n",
        "$$\n",
        "\n",
        "where $Z^T$ is the transpose of matrix $Z$.\n",
        "\n",
        "### Singular Value Decomposition (SVD)\n",
        "\n",
        "We apply Singular Value Decomposition to the correlation matrix $C$:\n",
        "\n",
        "$$\n",
        "C = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "Here, $U$ and $V$ are orthogonal matrices representing the left and right singular vectors, respectively, and $\\Sigma$ is a diagonal matrix containing the singular values, which are the square roots of the eigenvalues.\n",
        "\n",
        "### Construction of Hidden Factors\n",
        "\n",
        "For each of the top $m$ components, we construct the selected hidden factors as follows:\n",
        "\n",
        "$$\n",
        "F_j^t = \\sum_{i=1}^N \\frac{\\lambda_{i,j}}{\\sigma_i} R_i^t\n",
        "$$\n",
        "\n",
        "where $\\lambda_{i,j}$ is the $i$-th component of the $j$-th eigenvector (ranked by eigenvalue magnitude).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIkNjUbFZ3Wy"
      },
      "source": [
        "---\n",
        "<font color=green>Q13 (3 Marks):\n",
        "\n",
        "For the specified period from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28'), generate the matrix $Z$ by standardizing the stock returns using the DataFrame `df_filtered_new`\n",
        "</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AqhmrY9xcbnk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The DataFrame's First Date is 2023-03-29 00:00:00-04:00, and Last Date is 2024-03-28 00:00:00-04:00\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>ADI</th>\n",
              "      <th>ANSS</th>\n",
              "      <th>...</th>\n",
              "      <th>TTWO</th>\n",
              "      <th>TMUS</th>\n",
              "      <th>TSLA</th>\n",
              "      <th>TXN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>WBA</th>\n",
              "      <th>WBD</th>\n",
              "      <th>WDAY</th>\n",
              "      <th>XEL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-29 00:00:00-04:00</th>\n",
              "      <td>0.658925</td>\n",
              "      <td>2.189515</td>\n",
              "      <td>0.106285</td>\n",
              "      <td>0.207821</td>\n",
              "      <td>1.503570</td>\n",
              "      <td>0.444433</td>\n",
              "      <td>1.020469</td>\n",
              "      <td>0.727404</td>\n",
              "      <td>1.860840</td>\n",
              "      <td>0.356414</td>\n",
              "      <td>...</td>\n",
              "      <td>0.517876</td>\n",
              "      <td>0.615256</td>\n",
              "      <td>0.814986</td>\n",
              "      <td>1.344644</td>\n",
              "      <td>1.105776</td>\n",
              "      <td>0.127319</td>\n",
              "      <td>0.497111</td>\n",
              "      <td>0.429736</td>\n",
              "      <td>2.277783</td>\n",
              "      <td>1.269703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30 00:00:00-04:00</th>\n",
              "      <td>0.273051</td>\n",
              "      <td>-0.221312</td>\n",
              "      <td>-0.389329</td>\n",
              "      <td>-0.434766</td>\n",
              "      <td>0.787034</td>\n",
              "      <td>0.526996</td>\n",
              "      <td>0.277318</td>\n",
              "      <td>0.076712</td>\n",
              "      <td>1.630406</td>\n",
              "      <td>0.936497</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109884</td>\n",
              "      <td>0.439952</td>\n",
              "      <td>0.233493</td>\n",
              "      <td>1.187056</td>\n",
              "      <td>0.239468</td>\n",
              "      <td>-0.525980</td>\n",
              "      <td>0.691233</td>\n",
              "      <td>0.446100</td>\n",
              "      <td>0.389915</td>\n",
              "      <td>0.430698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31 00:00:00-04:00</th>\n",
              "      <td>0.360570</td>\n",
              "      <td>1.136308</td>\n",
              "      <td>1.540732</td>\n",
              "      <td>1.439604</td>\n",
              "      <td>0.531735</td>\n",
              "      <td>-0.056439</td>\n",
              "      <td>0.475342</td>\n",
              "      <td>0.008643</td>\n",
              "      <td>0.935425</td>\n",
              "      <td>1.044069</td>\n",
              "      <td>...</td>\n",
              "      <td>1.337544</td>\n",
              "      <td>0.117644</td>\n",
              "      <td>2.058867</td>\n",
              "      <td>0.640231</td>\n",
              "      <td>0.325257</td>\n",
              "      <td>0.538943</td>\n",
              "      <td>-0.008813</td>\n",
              "      <td>0.565842</td>\n",
              "      <td>1.611597</td>\n",
              "      <td>0.597554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-03 00:00:00-04:00</th>\n",
              "      <td>-0.713053</td>\n",
              "      <td>-2.259585</td>\n",
              "      <td>0.252735</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>-0.591892</td>\n",
              "      <td>-0.600163</td>\n",
              "      <td>-0.086824</td>\n",
              "      <td>0.759725</td>\n",
              "      <td>-0.328370</td>\n",
              "      <td>-0.555499</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.377687</td>\n",
              "      <td>1.191785</td>\n",
              "      <td>-2.030038</td>\n",
              "      <td>-0.684861</td>\n",
              "      <td>-0.214453</td>\n",
              "      <td>0.183344</td>\n",
              "      <td>1.205797</td>\n",
              "      <td>-0.547975</td>\n",
              "      <td>-0.634285</td>\n",
              "      <td>0.121519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-04 00:00:00-04:00</th>\n",
              "      <td>0.560730</td>\n",
              "      <td>-1.137430</td>\n",
              "      <td>0.099652</td>\n",
              "      <td>0.013877</td>\n",
              "      <td>0.658632</td>\n",
              "      <td>-0.342217</td>\n",
              "      <td>0.223115</td>\n",
              "      <td>0.872397</td>\n",
              "      <td>-0.399706</td>\n",
              "      <td>-0.127907</td>\n",
              "      <td>...</td>\n",
              "      <td>1.434972</td>\n",
              "      <td>-0.347683</td>\n",
              "      <td>-0.377655</td>\n",
              "      <td>-1.394525</td>\n",
              "      <td>-0.538712</td>\n",
              "      <td>-0.487155</td>\n",
              "      <td>0.553147</td>\n",
              "      <td>0.755053</td>\n",
              "      <td>-0.537767</td>\n",
              "      <td>1.011205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               ADBE       ADP     GOOGL      GOOG      AMZN  \\\n",
              "Date                                                                          \n",
              "2023-03-29 00:00:00-04:00  0.658925  2.189515  0.106285  0.207821  1.503570   \n",
              "2023-03-30 00:00:00-04:00  0.273051 -0.221312 -0.389329 -0.434766  0.787034   \n",
              "2023-03-31 00:00:00-04:00  0.360570  1.136308  1.540732  1.439604  0.531735   \n",
              "2023-04-03 00:00:00-04:00 -0.713053 -2.259585  0.252735  0.407399 -0.591892   \n",
              "2023-04-04 00:00:00-04:00  0.560730 -1.137430  0.099652  0.013877  0.658632   \n",
              "\n",
              "                                AMD       AEP      AMGN       ADI      ANSS  \\\n",
              "Date                                                                          \n",
              "2023-03-29 00:00:00-04:00  0.444433  1.020469  0.727404  1.860840  0.356414   \n",
              "2023-03-30 00:00:00-04:00  0.526996  0.277318  0.076712  1.630406  0.936497   \n",
              "2023-03-31 00:00:00-04:00 -0.056439  0.475342  0.008643  0.935425  1.044069   \n",
              "2023-04-03 00:00:00-04:00 -0.600163 -0.086824  0.759725 -0.328370 -0.555499   \n",
              "2023-04-04 00:00:00-04:00 -0.342217  0.223115  0.872397 -0.399706 -0.127907   \n",
              "\n",
              "                           ...      TTWO      TMUS      TSLA       TXN  \\\n",
              "Date                       ...                                           \n",
              "2023-03-29 00:00:00-04:00  ...  0.517876  0.615256  0.814986  1.344644   \n",
              "2023-03-30 00:00:00-04:00  ... -0.109884  0.439952  0.233493  1.187056   \n",
              "2023-03-31 00:00:00-04:00  ...  1.337544  0.117644  2.058867  0.640231   \n",
              "2023-04-03 00:00:00-04:00  ... -0.377687  1.191785 -2.030038 -0.684861   \n",
              "2023-04-04 00:00:00-04:00  ...  1.434972 -0.347683 -0.377655 -1.394525   \n",
              "\n",
              "                               VRSK      VRTX       WBA       WBD      WDAY  \\\n",
              "Date                                                                          \n",
              "2023-03-29 00:00:00-04:00  1.105776  0.127319  0.497111  0.429736  2.277783   \n",
              "2023-03-30 00:00:00-04:00  0.239468 -0.525980  0.691233  0.446100  0.389915   \n",
              "2023-03-31 00:00:00-04:00  0.325257  0.538943 -0.008813  0.565842  1.611597   \n",
              "2023-04-03 00:00:00-04:00 -0.214453  0.183344  1.205797 -0.547975 -0.634285   \n",
              "2023-04-04 00:00:00-04:00 -0.538712 -0.487155  0.553147  0.755053 -0.537767   \n",
              "\n",
              "                                XEL  \n",
              "Date                                 \n",
              "2023-03-29 00:00:00-04:00  1.269703  \n",
              "2023-03-30 00:00:00-04:00  0.430698  \n",
              "2023-03-31 00:00:00-04:00  0.597554  \n",
              "2023-04-03 00:00:00-04:00  0.121519  \n",
              "2023-04-04 00:00:00-04:00  1.011205  \n",
              "\n",
              "[5 rows x 89 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "start_date = '2023-03-29' # Start Date for Filtering Data\n",
        "end_date = '2024-03-28' # End Date for Filtering Data\n",
        "df_filtered_new = df_filtered.loc[start_date: end_date] # Put Filter Data into a New DataFrame\n",
        "print(f\"The DataFrame's First Date is {df_filtered_new.index[0]}, and Last Date is {df_filtered_new.index[-1]}\") # Sanity Check\n",
        "Z = (df_filtered_new - df_filtered_new.mean()) / df_filtered_new.std() # Create Z Matrix\n",
        "Z.head() # Display Z Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quv2tOhXch4-"
      },
      "source": [
        "---\n",
        "<font color=green>Q14: (1 Mark) </font>\n",
        "<br><font color='green'>\n",
        "Download the `Z_matrix` matrix from the course's GitHub account.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NmSz-J_oxYQa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>ADI</th>\n",
              "      <th>ANSS</th>\n",
              "      <th>...</th>\n",
              "      <th>TTWO</th>\n",
              "      <th>TMUS</th>\n",
              "      <th>TSLA</th>\n",
              "      <th>TXN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>WBA</th>\n",
              "      <th>WBD</th>\n",
              "      <th>WDAY</th>\n",
              "      <th>XEL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>0.658925</td>\n",
              "      <td>2.189521</td>\n",
              "      <td>0.106285</td>\n",
              "      <td>0.207821</td>\n",
              "      <td>1.503570</td>\n",
              "      <td>0.444433</td>\n",
              "      <td>1.020467</td>\n",
              "      <td>0.727398</td>\n",
              "      <td>1.860839</td>\n",
              "      <td>0.356414</td>\n",
              "      <td>...</td>\n",
              "      <td>0.517876</td>\n",
              "      <td>0.615242</td>\n",
              "      <td>0.814986</td>\n",
              "      <td>1.344650</td>\n",
              "      <td>1.105776</td>\n",
              "      <td>0.127319</td>\n",
              "      <td>0.497115</td>\n",
              "      <td>0.429736</td>\n",
              "      <td>2.277783</td>\n",
              "      <td>1.269711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>0.273051</td>\n",
              "      <td>-0.221306</td>\n",
              "      <td>-0.389329</td>\n",
              "      <td>-0.434766</td>\n",
              "      <td>0.787034</td>\n",
              "      <td>0.526996</td>\n",
              "      <td>0.277291</td>\n",
              "      <td>0.076716</td>\n",
              "      <td>1.630406</td>\n",
              "      <td>0.936497</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109884</td>\n",
              "      <td>0.439972</td>\n",
              "      <td>0.233493</td>\n",
              "      <td>1.187056</td>\n",
              "      <td>0.239468</td>\n",
              "      <td>-0.525980</td>\n",
              "      <td>0.691243</td>\n",
              "      <td>0.446100</td>\n",
              "      <td>0.389915</td>\n",
              "      <td>0.430715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>0.360570</td>\n",
              "      <td>1.136308</td>\n",
              "      <td>1.540732</td>\n",
              "      <td>1.439604</td>\n",
              "      <td>0.531735</td>\n",
              "      <td>-0.056439</td>\n",
              "      <td>0.475365</td>\n",
              "      <td>0.008639</td>\n",
              "      <td>0.935425</td>\n",
              "      <td>1.044069</td>\n",
              "      <td>...</td>\n",
              "      <td>1.337544</td>\n",
              "      <td>0.117646</td>\n",
              "      <td>2.058867</td>\n",
              "      <td>0.640237</td>\n",
              "      <td>0.325258</td>\n",
              "      <td>0.538943</td>\n",
              "      <td>-0.008816</td>\n",
              "      <td>0.565842</td>\n",
              "      <td>1.611597</td>\n",
              "      <td>0.597545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-03</th>\n",
              "      <td>-0.713053</td>\n",
              "      <td>-2.259591</td>\n",
              "      <td>0.252735</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>-0.591892</td>\n",
              "      <td>-0.600163</td>\n",
              "      <td>-0.086824</td>\n",
              "      <td>0.759731</td>\n",
              "      <td>-0.328378</td>\n",
              "      <td>-0.555499</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.377687</td>\n",
              "      <td>1.191774</td>\n",
              "      <td>-2.030038</td>\n",
              "      <td>-0.684867</td>\n",
              "      <td>-0.214460</td>\n",
              "      <td>0.183344</td>\n",
              "      <td>1.205798</td>\n",
              "      <td>-0.547975</td>\n",
              "      <td>-0.634285</td>\n",
              "      <td>0.121519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-04</th>\n",
              "      <td>0.560730</td>\n",
              "      <td>-1.137430</td>\n",
              "      <td>0.099652</td>\n",
              "      <td>0.013877</td>\n",
              "      <td>0.658632</td>\n",
              "      <td>-0.342217</td>\n",
              "      <td>0.223111</td>\n",
              "      <td>0.872403</td>\n",
              "      <td>-0.399698</td>\n",
              "      <td>-0.127907</td>\n",
              "      <td>...</td>\n",
              "      <td>1.434972</td>\n",
              "      <td>-0.347689</td>\n",
              "      <td>-0.377655</td>\n",
              "      <td>-1.394525</td>\n",
              "      <td>-0.538698</td>\n",
              "      <td>-0.487155</td>\n",
              "      <td>0.553153</td>\n",
              "      <td>0.755053</td>\n",
              "      <td>-0.537767</td>\n",
              "      <td>1.011197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
              "Date                                                                     \n",
              "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
              "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
              "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
              "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
              "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
              "\n",
              "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
              "Date                                                ...                       \n",
              "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
              "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
              "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
              "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
              "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
              "\n",
              "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
              "Date                                                                     \n",
              "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
              "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
              "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
              "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
              "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
              "\n",
              "                WDAY       XEL  \n",
              "Date                            \n",
              "2023-03-29  2.277783  1.269711  \n",
              "2023-03-30  0.389915  0.430715  \n",
              "2023-03-31  1.611597  0.597545  \n",
              "2023-04-03 -0.634285  0.121519  \n",
              "2023-04-04 -0.537767  1.011197  \n",
              "\n",
              "[5 rows x 89 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "# Get Z Matrix via GitHub Path\n",
        "Z_matrix_github_path = r\"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_matrix.csv\"\n",
        "Z_matrix = pd.read_csv(Z_matrix_github_path, index_col=0)\n",
        "Z_matrix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Z_Matrix via GitHub matches the Z Matrix created earlier manually via standardization. This confirms that the Z Matrix in Q13 was calculated correctly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d-2MrrzatMc"
      },
      "source": [
        "---\n",
        "<font color=green>Q15: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "For the specified period from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28'), compute the correlation matrix\n",
        "$C$ using the matrix `Z_matrix`.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2rRt-HL1xZqA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>ADI</th>\n",
              "      <th>ANSS</th>\n",
              "      <th>...</th>\n",
              "      <th>TTWO</th>\n",
              "      <th>TMUS</th>\n",
              "      <th>TSLA</th>\n",
              "      <th>TXN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>WBA</th>\n",
              "      <th>WBD</th>\n",
              "      <th>WDAY</th>\n",
              "      <th>XEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ADBE</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.218513</td>\n",
              "      <td>0.397890</td>\n",
              "      <td>0.400601</td>\n",
              "      <td>0.463488</td>\n",
              "      <td>0.444032</td>\n",
              "      <td>-0.035967</td>\n",
              "      <td>0.198781</td>\n",
              "      <td>0.321991</td>\n",
              "      <td>0.387483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257931</td>\n",
              "      <td>0.102167</td>\n",
              "      <td>0.268863</td>\n",
              "      <td>0.326597</td>\n",
              "      <td>0.171580</td>\n",
              "      <td>0.164760</td>\n",
              "      <td>0.033955</td>\n",
              "      <td>0.099841</td>\n",
              "      <td>0.418110</td>\n",
              "      <td>0.019105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADP</th>\n",
              "      <td>0.218513</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294213</td>\n",
              "      <td>0.298841</td>\n",
              "      <td>0.168206</td>\n",
              "      <td>0.045884</td>\n",
              "      <td>0.228457</td>\n",
              "      <td>0.214813</td>\n",
              "      <td>0.279607</td>\n",
              "      <td>0.238355</td>\n",
              "      <td>...</td>\n",
              "      <td>0.290311</td>\n",
              "      <td>0.113985</td>\n",
              "      <td>0.178128</td>\n",
              "      <td>0.297954</td>\n",
              "      <td>0.325258</td>\n",
              "      <td>0.176771</td>\n",
              "      <td>0.142369</td>\n",
              "      <td>0.243986</td>\n",
              "      <td>0.320836</td>\n",
              "      <td>0.164682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GOOGL</th>\n",
              "      <td>0.397890</td>\n",
              "      <td>0.294213</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997415</td>\n",
              "      <td>0.521199</td>\n",
              "      <td>0.371105</td>\n",
              "      <td>-0.006803</td>\n",
              "      <td>0.118938</td>\n",
              "      <td>0.222252</td>\n",
              "      <td>0.292286</td>\n",
              "      <td>...</td>\n",
              "      <td>0.238219</td>\n",
              "      <td>0.086673</td>\n",
              "      <td>0.267941</td>\n",
              "      <td>0.192188</td>\n",
              "      <td>0.178622</td>\n",
              "      <td>0.142447</td>\n",
              "      <td>0.052710</td>\n",
              "      <td>0.042072</td>\n",
              "      <td>0.289137</td>\n",
              "      <td>0.025701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GOOG</th>\n",
              "      <td>0.400601</td>\n",
              "      <td>0.298841</td>\n",
              "      <td>0.997415</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.525626</td>\n",
              "      <td>0.371568</td>\n",
              "      <td>-0.004037</td>\n",
              "      <td>0.118296</td>\n",
              "      <td>0.223710</td>\n",
              "      <td>0.294542</td>\n",
              "      <td>...</td>\n",
              "      <td>0.242111</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.268114</td>\n",
              "      <td>0.198044</td>\n",
              "      <td>0.180110</td>\n",
              "      <td>0.146190</td>\n",
              "      <td>0.060822</td>\n",
              "      <td>0.045516</td>\n",
              "      <td>0.293575</td>\n",
              "      <td>0.026392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AMZN</th>\n",
              "      <td>0.463488</td>\n",
              "      <td>0.168206</td>\n",
              "      <td>0.521199</td>\n",
              "      <td>0.525626</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.463049</td>\n",
              "      <td>-0.010849</td>\n",
              "      <td>0.123745</td>\n",
              "      <td>0.290872</td>\n",
              "      <td>0.342042</td>\n",
              "      <td>...</td>\n",
              "      <td>0.222346</td>\n",
              "      <td>0.120301</td>\n",
              "      <td>0.303368</td>\n",
              "      <td>0.299500</td>\n",
              "      <td>0.144325</td>\n",
              "      <td>0.104962</td>\n",
              "      <td>0.017926</td>\n",
              "      <td>0.162937</td>\n",
              "      <td>0.403757</td>\n",
              "      <td>-0.058870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VRTX</th>\n",
              "      <td>0.164760</td>\n",
              "      <td>0.176771</td>\n",
              "      <td>0.142447</td>\n",
              "      <td>0.146190</td>\n",
              "      <td>0.104962</td>\n",
              "      <td>0.039540</td>\n",
              "      <td>0.239861</td>\n",
              "      <td>0.281759</td>\n",
              "      <td>0.110189</td>\n",
              "      <td>0.142121</td>\n",
              "      <td>...</td>\n",
              "      <td>0.180810</td>\n",
              "      <td>0.139184</td>\n",
              "      <td>0.144443</td>\n",
              "      <td>0.198258</td>\n",
              "      <td>0.251863</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.159124</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.101851</td>\n",
              "      <td>0.184369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WBA</th>\n",
              "      <td>0.033955</td>\n",
              "      <td>0.142369</td>\n",
              "      <td>0.052710</td>\n",
              "      <td>0.060822</td>\n",
              "      <td>0.017926</td>\n",
              "      <td>0.002629</td>\n",
              "      <td>0.309717</td>\n",
              "      <td>0.214701</td>\n",
              "      <td>0.208907</td>\n",
              "      <td>0.096813</td>\n",
              "      <td>...</td>\n",
              "      <td>0.115189</td>\n",
              "      <td>0.063538</td>\n",
              "      <td>0.168495</td>\n",
              "      <td>0.199627</td>\n",
              "      <td>0.038371</td>\n",
              "      <td>0.159124</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.361533</td>\n",
              "      <td>0.010855</td>\n",
              "      <td>0.194839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WBD</th>\n",
              "      <td>0.099841</td>\n",
              "      <td>0.243986</td>\n",
              "      <td>0.042072</td>\n",
              "      <td>0.045516</td>\n",
              "      <td>0.162937</td>\n",
              "      <td>0.092733</td>\n",
              "      <td>0.325463</td>\n",
              "      <td>0.220342</td>\n",
              "      <td>0.310681</td>\n",
              "      <td>0.095919</td>\n",
              "      <td>...</td>\n",
              "      <td>0.129023</td>\n",
              "      <td>0.084315</td>\n",
              "      <td>0.282265</td>\n",
              "      <td>0.355450</td>\n",
              "      <td>0.002990</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.361533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.160860</td>\n",
              "      <td>0.183837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDAY</th>\n",
              "      <td>0.418110</td>\n",
              "      <td>0.320836</td>\n",
              "      <td>0.289137</td>\n",
              "      <td>0.293575</td>\n",
              "      <td>0.403757</td>\n",
              "      <td>0.334587</td>\n",
              "      <td>0.017659</td>\n",
              "      <td>0.068097</td>\n",
              "      <td>0.315426</td>\n",
              "      <td>0.382360</td>\n",
              "      <td>...</td>\n",
              "      <td>0.293949</td>\n",
              "      <td>0.142648</td>\n",
              "      <td>0.277744</td>\n",
              "      <td>0.346161</td>\n",
              "      <td>0.195588</td>\n",
              "      <td>0.101851</td>\n",
              "      <td>0.010855</td>\n",
              "      <td>0.160860</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.019310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XEL</th>\n",
              "      <td>0.019105</td>\n",
              "      <td>0.164682</td>\n",
              "      <td>0.025701</td>\n",
              "      <td>0.026392</td>\n",
              "      <td>-0.058870</td>\n",
              "      <td>-0.214673</td>\n",
              "      <td>0.617318</td>\n",
              "      <td>0.283399</td>\n",
              "      <td>0.029036</td>\n",
              "      <td>0.072063</td>\n",
              "      <td>...</td>\n",
              "      <td>0.074596</td>\n",
              "      <td>0.249106</td>\n",
              "      <td>0.023168</td>\n",
              "      <td>0.062496</td>\n",
              "      <td>0.151549</td>\n",
              "      <td>0.184369</td>\n",
              "      <td>0.194839</td>\n",
              "      <td>0.183837</td>\n",
              "      <td>-0.019310</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows × 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           ADBE       ADP     GOOGL      GOOG      AMZN       AMD       AEP  \\\n",
              "ADBE   1.000000  0.218513  0.397890  0.400601  0.463488  0.444032 -0.035967   \n",
              "ADP    0.218513  1.000000  0.294213  0.298841  0.168206  0.045884  0.228457   \n",
              "GOOGL  0.397890  0.294213  1.000000  0.997415  0.521199  0.371105 -0.006803   \n",
              "GOOG   0.400601  0.298841  0.997415  1.000000  0.525626  0.371568 -0.004037   \n",
              "AMZN   0.463488  0.168206  0.521199  0.525626  1.000000  0.463049 -0.010849   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "VRTX   0.164760  0.176771  0.142447  0.146190  0.104962  0.039540  0.239861   \n",
              "WBA    0.033955  0.142369  0.052710  0.060822  0.017926  0.002629  0.309717   \n",
              "WBD    0.099841  0.243986  0.042072  0.045516  0.162937  0.092733  0.325463   \n",
              "WDAY   0.418110  0.320836  0.289137  0.293575  0.403757  0.334587  0.017659   \n",
              "XEL    0.019105  0.164682  0.025701  0.026392 -0.058870 -0.214673  0.617318   \n",
              "\n",
              "           AMGN       ADI      ANSS  ...      TTWO      TMUS      TSLA  \\\n",
              "ADBE   0.198781  0.321991  0.387483  ...  0.257931  0.102167  0.268863   \n",
              "ADP    0.214813  0.279607  0.238355  ...  0.290311  0.113985  0.178128   \n",
              "GOOGL  0.118938  0.222252  0.292286  ...  0.238219  0.086673  0.267941   \n",
              "GOOG   0.118296  0.223710  0.294542  ...  0.242111  0.091456  0.268114   \n",
              "AMZN   0.123745  0.290872  0.342042  ...  0.222346  0.120301  0.303368   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "VRTX   0.281759  0.110189  0.142121  ...  0.180810  0.139184  0.144443   \n",
              "WBA    0.214701  0.208907  0.096813  ...  0.115189  0.063538  0.168495   \n",
              "WBD    0.220342  0.310681  0.095919  ...  0.129023  0.084315  0.282265   \n",
              "WDAY   0.068097  0.315426  0.382360  ...  0.293949  0.142648  0.277744   \n",
              "XEL    0.283399  0.029036  0.072063  ...  0.074596  0.249106  0.023168   \n",
              "\n",
              "            TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
              "ADBE   0.326597  0.171580  0.164760  0.033955  0.099841  0.418110  0.019105  \n",
              "ADP    0.297954  0.325258  0.176771  0.142369  0.243986  0.320836  0.164682  \n",
              "GOOGL  0.192188  0.178622  0.142447  0.052710  0.042072  0.289137  0.025701  \n",
              "GOOG   0.198044  0.180110  0.146190  0.060822  0.045516  0.293575  0.026392  \n",
              "AMZN   0.299500  0.144325  0.104962  0.017926  0.162937  0.403757 -0.058870  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "VRTX   0.198258  0.251863  1.000000  0.159124  0.062726  0.101851  0.184369  \n",
              "WBA    0.199627  0.038371  0.159124  1.000000  0.361533  0.010855  0.194839  \n",
              "WBD    0.355450  0.002990  0.062726  0.361533  1.000000  0.160860  0.183837  \n",
              "WDAY   0.346161  0.195588  0.101851  0.010855  0.160860  1.000000 -0.019310  \n",
              "XEL    0.062496  0.151549  0.184369  0.194839  0.183837 -0.019310  1.000000  \n",
              "\n",
              "[89 rows x 89 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "T = len(Z_matrix) # Length of Z Matrix\n",
        "C_formula = (1/(T - 1)) * Z_matrix.T @ Z_matrix # Create Empirical Correlation Matrix\n",
        "C_formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FnUvUEkjAbG"
      },
      "source": [
        "---\n",
        "<font color=green>Q16: (2 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Refind the correlation matrix from the from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28') using pandas correlation matrix method.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "T_g-VjITxasb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>ADI</th>\n",
              "      <th>ANSS</th>\n",
              "      <th>...</th>\n",
              "      <th>TTWO</th>\n",
              "      <th>TMUS</th>\n",
              "      <th>TSLA</th>\n",
              "      <th>TXN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>WBA</th>\n",
              "      <th>WBD</th>\n",
              "      <th>WDAY</th>\n",
              "      <th>XEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ADBE</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.218513</td>\n",
              "      <td>0.397890</td>\n",
              "      <td>0.400601</td>\n",
              "      <td>0.463488</td>\n",
              "      <td>0.444032</td>\n",
              "      <td>-0.035967</td>\n",
              "      <td>0.198781</td>\n",
              "      <td>0.321991</td>\n",
              "      <td>0.387483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257931</td>\n",
              "      <td>0.102167</td>\n",
              "      <td>0.268863</td>\n",
              "      <td>0.326597</td>\n",
              "      <td>0.171580</td>\n",
              "      <td>0.164760</td>\n",
              "      <td>0.033955</td>\n",
              "      <td>0.099841</td>\n",
              "      <td>0.418110</td>\n",
              "      <td>0.019105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADP</th>\n",
              "      <td>0.218513</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294213</td>\n",
              "      <td>0.298841</td>\n",
              "      <td>0.168206</td>\n",
              "      <td>0.045884</td>\n",
              "      <td>0.228457</td>\n",
              "      <td>0.214813</td>\n",
              "      <td>0.279607</td>\n",
              "      <td>0.238355</td>\n",
              "      <td>...</td>\n",
              "      <td>0.290311</td>\n",
              "      <td>0.113985</td>\n",
              "      <td>0.178128</td>\n",
              "      <td>0.297954</td>\n",
              "      <td>0.325258</td>\n",
              "      <td>0.176771</td>\n",
              "      <td>0.142369</td>\n",
              "      <td>0.243986</td>\n",
              "      <td>0.320836</td>\n",
              "      <td>0.164682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GOOGL</th>\n",
              "      <td>0.397890</td>\n",
              "      <td>0.294213</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997415</td>\n",
              "      <td>0.521199</td>\n",
              "      <td>0.371105</td>\n",
              "      <td>-0.006803</td>\n",
              "      <td>0.118938</td>\n",
              "      <td>0.222252</td>\n",
              "      <td>0.292286</td>\n",
              "      <td>...</td>\n",
              "      <td>0.238219</td>\n",
              "      <td>0.086673</td>\n",
              "      <td>0.267941</td>\n",
              "      <td>0.192188</td>\n",
              "      <td>0.178622</td>\n",
              "      <td>0.142447</td>\n",
              "      <td>0.052710</td>\n",
              "      <td>0.042072</td>\n",
              "      <td>0.289137</td>\n",
              "      <td>0.025701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GOOG</th>\n",
              "      <td>0.400601</td>\n",
              "      <td>0.298841</td>\n",
              "      <td>0.997415</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.525626</td>\n",
              "      <td>0.371568</td>\n",
              "      <td>-0.004037</td>\n",
              "      <td>0.118296</td>\n",
              "      <td>0.223710</td>\n",
              "      <td>0.294542</td>\n",
              "      <td>...</td>\n",
              "      <td>0.242111</td>\n",
              "      <td>0.091456</td>\n",
              "      <td>0.268114</td>\n",
              "      <td>0.198044</td>\n",
              "      <td>0.180110</td>\n",
              "      <td>0.146190</td>\n",
              "      <td>0.060822</td>\n",
              "      <td>0.045516</td>\n",
              "      <td>0.293575</td>\n",
              "      <td>0.026392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AMZN</th>\n",
              "      <td>0.463488</td>\n",
              "      <td>0.168206</td>\n",
              "      <td>0.521199</td>\n",
              "      <td>0.525626</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.463049</td>\n",
              "      <td>-0.010849</td>\n",
              "      <td>0.123745</td>\n",
              "      <td>0.290872</td>\n",
              "      <td>0.342042</td>\n",
              "      <td>...</td>\n",
              "      <td>0.222346</td>\n",
              "      <td>0.120301</td>\n",
              "      <td>0.303368</td>\n",
              "      <td>0.299500</td>\n",
              "      <td>0.144325</td>\n",
              "      <td>0.104962</td>\n",
              "      <td>0.017926</td>\n",
              "      <td>0.162937</td>\n",
              "      <td>0.403757</td>\n",
              "      <td>-0.058870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VRTX</th>\n",
              "      <td>0.164760</td>\n",
              "      <td>0.176771</td>\n",
              "      <td>0.142447</td>\n",
              "      <td>0.146190</td>\n",
              "      <td>0.104962</td>\n",
              "      <td>0.039540</td>\n",
              "      <td>0.239861</td>\n",
              "      <td>0.281759</td>\n",
              "      <td>0.110189</td>\n",
              "      <td>0.142121</td>\n",
              "      <td>...</td>\n",
              "      <td>0.180810</td>\n",
              "      <td>0.139184</td>\n",
              "      <td>0.144443</td>\n",
              "      <td>0.198258</td>\n",
              "      <td>0.251863</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.159124</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.101851</td>\n",
              "      <td>0.184369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WBA</th>\n",
              "      <td>0.033955</td>\n",
              "      <td>0.142369</td>\n",
              "      <td>0.052710</td>\n",
              "      <td>0.060822</td>\n",
              "      <td>0.017926</td>\n",
              "      <td>0.002629</td>\n",
              "      <td>0.309717</td>\n",
              "      <td>0.214701</td>\n",
              "      <td>0.208907</td>\n",
              "      <td>0.096813</td>\n",
              "      <td>...</td>\n",
              "      <td>0.115189</td>\n",
              "      <td>0.063538</td>\n",
              "      <td>0.168495</td>\n",
              "      <td>0.199627</td>\n",
              "      <td>0.038371</td>\n",
              "      <td>0.159124</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.361533</td>\n",
              "      <td>0.010855</td>\n",
              "      <td>0.194839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WBD</th>\n",
              "      <td>0.099841</td>\n",
              "      <td>0.243986</td>\n",
              "      <td>0.042072</td>\n",
              "      <td>0.045516</td>\n",
              "      <td>0.162937</td>\n",
              "      <td>0.092733</td>\n",
              "      <td>0.325463</td>\n",
              "      <td>0.220342</td>\n",
              "      <td>0.310681</td>\n",
              "      <td>0.095919</td>\n",
              "      <td>...</td>\n",
              "      <td>0.129023</td>\n",
              "      <td>0.084315</td>\n",
              "      <td>0.282265</td>\n",
              "      <td>0.355450</td>\n",
              "      <td>0.002990</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.361533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.160860</td>\n",
              "      <td>0.183837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WDAY</th>\n",
              "      <td>0.418110</td>\n",
              "      <td>0.320836</td>\n",
              "      <td>0.289137</td>\n",
              "      <td>0.293575</td>\n",
              "      <td>0.403757</td>\n",
              "      <td>0.334587</td>\n",
              "      <td>0.017659</td>\n",
              "      <td>0.068097</td>\n",
              "      <td>0.315426</td>\n",
              "      <td>0.382360</td>\n",
              "      <td>...</td>\n",
              "      <td>0.293949</td>\n",
              "      <td>0.142648</td>\n",
              "      <td>0.277744</td>\n",
              "      <td>0.346161</td>\n",
              "      <td>0.195588</td>\n",
              "      <td>0.101851</td>\n",
              "      <td>0.010855</td>\n",
              "      <td>0.160860</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.019310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XEL</th>\n",
              "      <td>0.019105</td>\n",
              "      <td>0.164682</td>\n",
              "      <td>0.025701</td>\n",
              "      <td>0.026392</td>\n",
              "      <td>-0.058870</td>\n",
              "      <td>-0.214673</td>\n",
              "      <td>0.617318</td>\n",
              "      <td>0.283399</td>\n",
              "      <td>0.029036</td>\n",
              "      <td>0.072063</td>\n",
              "      <td>...</td>\n",
              "      <td>0.074596</td>\n",
              "      <td>0.249106</td>\n",
              "      <td>0.023168</td>\n",
              "      <td>0.062496</td>\n",
              "      <td>0.151549</td>\n",
              "      <td>0.184369</td>\n",
              "      <td>0.194839</td>\n",
              "      <td>0.183837</td>\n",
              "      <td>-0.019310</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows × 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           ADBE       ADP     GOOGL      GOOG      AMZN       AMD       AEP  \\\n",
              "ADBE   1.000000  0.218513  0.397890  0.400601  0.463488  0.444032 -0.035967   \n",
              "ADP    0.218513  1.000000  0.294213  0.298841  0.168206  0.045884  0.228457   \n",
              "GOOGL  0.397890  0.294213  1.000000  0.997415  0.521199  0.371105 -0.006803   \n",
              "GOOG   0.400601  0.298841  0.997415  1.000000  0.525626  0.371568 -0.004037   \n",
              "AMZN   0.463488  0.168206  0.521199  0.525626  1.000000  0.463049 -0.010849   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "VRTX   0.164760  0.176771  0.142447  0.146190  0.104962  0.039540  0.239861   \n",
              "WBA    0.033955  0.142369  0.052710  0.060822  0.017926  0.002629  0.309717   \n",
              "WBD    0.099841  0.243986  0.042072  0.045516  0.162937  0.092733  0.325463   \n",
              "WDAY   0.418110  0.320836  0.289137  0.293575  0.403757  0.334587  0.017659   \n",
              "XEL    0.019105  0.164682  0.025701  0.026392 -0.058870 -0.214673  0.617318   \n",
              "\n",
              "           AMGN       ADI      ANSS  ...      TTWO      TMUS      TSLA  \\\n",
              "ADBE   0.198781  0.321991  0.387483  ...  0.257931  0.102167  0.268863   \n",
              "ADP    0.214813  0.279607  0.238355  ...  0.290311  0.113985  0.178128   \n",
              "GOOGL  0.118938  0.222252  0.292286  ...  0.238219  0.086673  0.267941   \n",
              "GOOG   0.118296  0.223710  0.294542  ...  0.242111  0.091456  0.268114   \n",
              "AMZN   0.123745  0.290872  0.342042  ...  0.222346  0.120301  0.303368   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "VRTX   0.281759  0.110189  0.142121  ...  0.180810  0.139184  0.144443   \n",
              "WBA    0.214701  0.208907  0.096813  ...  0.115189  0.063538  0.168495   \n",
              "WBD    0.220342  0.310681  0.095919  ...  0.129023  0.084315  0.282265   \n",
              "WDAY   0.068097  0.315426  0.382360  ...  0.293949  0.142648  0.277744   \n",
              "XEL    0.283399  0.029036  0.072063  ...  0.074596  0.249106  0.023168   \n",
              "\n",
              "            TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
              "ADBE   0.326597  0.171580  0.164760  0.033955  0.099841  0.418110  0.019105  \n",
              "ADP    0.297954  0.325258  0.176771  0.142369  0.243986  0.320836  0.164682  \n",
              "GOOGL  0.192188  0.178622  0.142447  0.052710  0.042072  0.289137  0.025701  \n",
              "GOOG   0.198044  0.180110  0.146190  0.060822  0.045516  0.293575  0.026392  \n",
              "AMZN   0.299500  0.144325  0.104962  0.017926  0.162937  0.403757 -0.058870  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "VRTX   0.198258  0.251863  1.000000  0.159124  0.062726  0.101851  0.184369  \n",
              "WBA    0.199627  0.038371  0.159124  1.000000  0.361533  0.010855  0.194839  \n",
              "WBD    0.355450  0.002990  0.062726  0.361533  1.000000  0.160860  0.183837  \n",
              "WDAY   0.346161  0.195588  0.101851  0.010855  0.160860  1.000000 -0.019310  \n",
              "XEL    0.062496  0.151549  0.184369  0.194839  0.183837 -0.019310  1.000000  \n",
              "\n",
              "[89 rows x 89 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "C = Z_matrix.corr()\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Values in the Empirical Correlation Matrix calculated via the Formula and via Pandas inherent method match, validating that the Correlation Matrix created via the Formula is correct. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvsiMvgfaxzW"
      },
      "source": [
        "---\n",
        "<font color=green>Q17: (7 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Conduct Singular Value Decomposition on the correlation matrix $C$. Follow these steps:\n",
        "\n",
        "\n",
        "1.   **Perform SVD**: Decompose the matrix $C$ into its singular values and vectors.\n",
        "2.   **Rank Eigenvalues**: Sort the resulting singular values (often squared to compare to eigenvalues) in descending order.\n",
        "3. **Select Components**: Extract the first 20 components based on the largest singular values.\n",
        "4. **Variance Explained**: Print the variance explained by the first 20 Components and dimensions of differents matrix that you created.\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Insert your code here\n",
        "# Performing SVD\n",
        "U, S, Vt = np.linalg.svd(C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pohRSmSExbkq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance explained by the first 20 components:\n",
            "[0.78617886 0.0864855  0.02462599 0.01223756 0.01065625 0.00673085\n",
            " 0.00554871 0.00498553 0.00486848 0.00417346 0.00369662 0.00336624\n",
            " 0.00315847 0.00281594 0.0025195  0.00228393 0.00220505 0.00211658\n",
            " 0.00191477 0.00185626]\n",
            "\n",
            "Dimensions of U matrix: (89, 89)\n",
            "Dimensions of singular values vector (S): (89,)\n",
            "Dimensions of Vt matrix: (89, 89)\n"
          ]
        }
      ],
      "source": [
        "# Ranking the Eigenvalues\n",
        "singular_values_squared = S**2 # Square the eigenvalues for comparison\n",
        "sorted_indices = np.argsort(singular_values_squared)[::-1] # Store the indices in Descending Order\n",
        "sorted_singular_values = singular_values_squared[sorted_indices] # Store the Singular Values in Descending Order\n",
        "\n",
        "# Select Top 20 Components\n",
        "top_20_components = sorted_singular_values[:20] # Use Slicing to select top 20 components\n",
        "\n",
        "# Variance Expained\n",
        "variance_explained = top_20_components / np.sum(singular_values_squared)\n",
        "\n",
        "# Printing results\n",
        "print(\"Variance explained by the first 20 components:\")\n",
        "print(variance_explained)\n",
        "\n",
        "# Printing dimensions of different matrices\n",
        "print(\"\\nDimensions of U matrix:\", U.shape)\n",
        "print(\"Dimensions of singular values vector (S):\", S.shape)\n",
        "print(\"Dimensions of Vt matrix:\", Vt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The above result seems reasonable as the Variance Explained by the First Principle Component is:\n",
            "78.62%\n",
            "The cumulative variance explained by top 20 Principle Components is:\n",
            "97.2 %\n"
          ]
        }
      ],
      "source": [
        "cumulative_variance_explained = np.sum(variance_explained)\n",
        "print(f\"The above result seems reasonable as the Variance Explained by the First Principle Component is:\")\n",
        "print(f\"{np.round(variance_explained[0], 4) * 100}%\")\n",
        "print(f\"The cumulative variance explained by top 20 Principle Components is:\")\n",
        "print(f\"{np.round(cumulative_variance_explained, 3) * 100} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sZ7K-Tb6S_"
      },
      "source": [
        "---\n",
        "<font color=green>Q18: (6 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Extract the 20 hidden factors in a matrix F. Check that shape of F is $(252,20)$\n",
        "</font>\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CzT32wMixcjq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(252, 20)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "sorted_Vt = Vt[sorted_indices, :] # Sort Eigenvectors based on sorted indices of eigenvalues\n",
        "V20 = sorted_Vt[:20, :].T # Take top 20 Eigenvectors\n",
        "\n",
        "std_devs = df_filtered_new.std().values\n",
        "normalized_V20 = V20 / std_devs[:, np.newaxis] # Normalize Top 20 Eigenvectors\n",
        "\n",
        "returns = df_filtered_new.values\n",
        "F = np.dot(returns, normalized_V20) # Create Hidden Factors Matrix\n",
        "F.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz-ncrUmCAVW"
      },
      "source": [
        "---\n",
        "<font color=green>Q19: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Perform the Regression Analysis of 'AAPL' for the date '2024-03-28', using a historical data length of 252 days using previous $F$ Matrix. Compare the R-squared from the ones obtained at Q11.\n",
        "</font>\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "aapl_returns = df_filtered_new['AAPL'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = F\n",
        "X = sm.add_constant(X)\n",
        "y = aapl_returns\n",
        "model = sm.OLS(y, X).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.615\n",
            "Model:                            OLS   Adj. R-squared:                  0.582\n",
            "Method:                 Least Squares   F-statistic:                     18.48\n",
            "Date:                Thu, 06 Jun 2024   Prob (F-statistic):           2.29e-37\n",
            "Time:                        20:14:40   Log-Likelihood:                 873.13\n",
            "No. Observations:                 252   AIC:                            -1704.\n",
            "Df Residuals:                     231   BIC:                            -1630.\n",
            "Df Model:                          20                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         -0.0005      0.001     -0.874      0.383      -0.001       0.001\n",
            "x1            -0.0015      0.000    -14.408      0.000      -0.002      -0.001\n",
            "x2             0.0002      0.000      0.981      0.327      -0.000       0.001\n",
            "x3             0.0012      0.000      4.741      0.000       0.001       0.002\n",
            "x4            -0.0003      0.000     -0.954      0.341      -0.001       0.000\n",
            "x5            -0.0014      0.000     -4.351      0.000      -0.002      -0.001\n",
            "x6            -0.0006      0.000     -1.632      0.104      -0.001       0.000\n",
            "x7            -0.0010      0.000     -2.862      0.005      -0.002      -0.000\n",
            "x8            -0.0024      0.000     -6.338      0.000      -0.003      -0.002\n",
            "x9          2.647e-05      0.000      0.070      0.944      -0.001       0.001\n",
            "x10           -0.0017      0.000     -4.295      0.000      -0.002      -0.001\n",
            "x11            0.0015      0.000      3.786      0.000       0.001       0.002\n",
            "x12           -0.0012      0.000     -2.866      0.005      -0.002      -0.000\n",
            "x13        -3.566e-05      0.000     -0.085      0.933      -0.001       0.001\n",
            "x14           -0.0004      0.000     -0.860      0.390      -0.001       0.000\n",
            "x15            0.0020      0.000      4.411      0.000       0.001       0.003\n",
            "x16            0.0007      0.000      1.490      0.138      -0.000       0.002\n",
            "x17            0.0006      0.000      1.202      0.231      -0.000       0.001\n",
            "x18            0.0004      0.000      0.915      0.361      -0.000       0.001\n",
            "x19           -0.0005      0.000     -1.138      0.256      -0.001       0.000\n",
            "x20           -0.0004      0.000     -0.853      0.395      -0.001       0.001\n",
            "==============================================================================\n",
            "Omnibus:                       14.605   Durbin-Watson:                   1.977\n",
            "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               25.420\n",
            "Skew:                          -0.323   Prob(JB):                     3.02e-06\n",
            "Kurtosis:                       4.415   Cond. No.                         5.07\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to the regression run in Q11, the R square obtained by the regression of hidden factors on AAPL Returns is higher. The R Squared obtained from Q11 was 0.385. Whereas the R square obtained here is 0.615, significantly higher than the previous result. PCA reduces the dimensionality of the returns dataset (Z Matrix) and transforms it into a set of orthogonal (uncorrelated) variables called principle components. These principal components catch most of the variance in the dataset as shown by the cumulative variance (97%) calculated in Q17. The regression of the Top 20 components on AAPL Returns show that they are able to explain 61.5% of the variability in the returns of AAPL as compared to 38.5% variability explained by the Fama French factors. The statistical technique PCA is able to catch more data specific nuances here and therefore performs better than the Fama French factors that are more broad about the market in general. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDU8xmpi_ueR"
      },
      "source": [
        "# Ornstein Uhlenbeck\n",
        "\n",
        "The Ornstein-Uhlenbeck process is defined by the following stochastic differential equation (SDE):\n",
        "\n",
        "$$ dX_t = \\theta (\\mu - X_t) dt + \\sigma dW_t $$\n",
        "\n",
        "where:\n",
        "\n",
        "- **$ X_t $**: The value of the process at time $ t $.\n",
        "- **$ \\mu $**: The long-term mean (equilibrium level) to which the process reverts.\n",
        "- **$ \\theta $**: The speed of reversion or the rate at which the process returns to the mean.\n",
        "- **$ \\sigma $**: The volatility (standard deviation), representing the magnitude of random fluctuations.\n",
        "- **$ W_t $**: A Wiener process or Brownian motion that adds stochastic (random) noise.\n",
        "\n",
        "This equation describes a process where the variable $ X_t $ moves towards the mean $ \\mu $ at a rate determined by $ \\theta $, with random noise added by $ \\sigma dW_t $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HMYOiWsP53c"
      },
      "source": [
        "---\n",
        "<font color=green>Q20: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "In the context of mean reversion, which quantity should be modeled using an Ornstein-Uhlenbeck process?\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiO01w7fO_bR"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The quantity that should be modelled is the residuals. The solution to the Ornstein-Uhlenbeck model is available in closed form. This tractability creates a significant advantage for statistical modelling. Moreover, one could emphasize on this point by stating that the mean and the variance are well defined. The process’s mean reverting nature ensures that the residuals tend to return to a long-term mean which should typically be zero, a common characteristic of residuals in a time series models. In addition, the residuals are assumed to be stochastic following a standard normal distribution. It is also important to mention that the mean-reverting property of the OU process sets limits for the drift ensuring that the process does not drift away indefinitely. In other words, it fluctuates around a stable equilibrium level controlled by the parameter theta. Theta controls how quickly residuals return to the mean (zero). Finally, it would be valuable to mention that the parameter sigma allows for modelling of the volatilities of the residuals through the OU process. This parameter gives an idea about the magnitude of the fluctuations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t31Q2iWgQgsO"
      },
      "source": [
        "---\n",
        "<font color=green>Q21: (5 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Explain how the parameters $ \\theta $ and $ \\sigma $ can be determined using the following equations. Also, detail the underlying assumptions:\n",
        "$$ E[X] = \\mu $$\n",
        "$$ \\text{Var}[X] = \\frac{\\sigma^2}{2\\theta} $$\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_KjMbQplj4U"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to explain how $\\theta$ and $\\mu$ can be determined using these equations, one will start by approximating the Ornstein-Uhlenbeck process using an AR(1) process to model $X$. Starting with the general form of the OU model, one can write the equation:\n",
        "\n",
        "$$ dX_t = \\theta (\\mu - X_t) dt + \\sigma dW_t $$\n",
        "\n",
        "Afterwards one will apply a discretization over a small interval $\\Delta t$. One obtains:\n",
        "$$ X_{t + \\Delta t} - X_t = \\theta (\\mu - X_t) \\Delta t + \\sigma \\sqrt{\\Delta t} \\cdot \\epsilon_t $$\n",
        "\n",
        "One should note that the increment of a Wiener process (when discretizing) can be approximated using the following formula:\n",
        "\n",
        "$$ W_{t + \\Delta t} - W_t \\approx \\sqrt{\\Delta t} \\cdot \\epsilon_t \\quad \\text{where } \\epsilon_t \\sim N(0, 1) $$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$ X_{t + \\Delta t} = X_t + \\theta (\\mu - X_t) \\Delta t + \\sigma \\sqrt{\\Delta t} \\cdot \\epsilon_t $$\n",
        "\n",
        "One knows that an AR(1) process has the form: \n",
        "\n",
        "$$ X_{t + 1} = \\phi X_t + c + \\epsilon_t $$\n",
        "\n",
        "\n",
        "Therefore, using the equation of $X_{t + \\Delta t}$ and the AR(1) process general formula, one can recover $\\phi$ and $c$. Thus,\n",
        "\n",
        "$$ \\phi = 1 - \\theta \\Delta t, \\quad c = \\theta \\mu \\Delta t $$\n",
        "\n",
        "One knows that for an AR(1) process the expected values and variance of $X$ are as follows:\n",
        "\n",
        "$$ E(X) = \\frac{c}{1 - \\phi} \\quad \\text{and} \\quad \\text{Var}(X) = \\frac{\\sigma^2}{1 - \\phi^2} $$\n",
        "\n",
        "From $E(X)$, $\\text{Var}(X)$ and the derived formulas of $\\phi$ and $c$, one will recover the equation of $E(X)$ and $\\text{Var}(X)$ for an OU process.\n",
        "\n",
        "$$ E(X) = \\frac{\\theta \\mu \\Delta t}{1 - (1 - \\theta \\Delta t)} = \\frac{\\theta \\mu \\Delta t}{\\theta \\Delta t} = \\mu \\implies E(X) = \\mu $$\n",
        "\n",
        "$$ \\text{Var}(X) = \\frac{\\sigma^2}{1 - \\phi^2} = \\frac{\\sigma^2}{1 - (1 - \\theta \\Delta t)^2} $$\n",
        "\n",
        "Using for small $\\Delta t$, one can approximate using the first Taylor expansion: \n",
        "$$(1 - \\theta \\Delta t)^2 \\approx 1 - 2 \\theta \\Delta t$$\n",
        "\n",
        "$$ \\implies \\text{Var}(X) = \\frac{\\sigma^2 \\Delta t}{1 - 1 + 2 \\theta \\Delta t} = \\frac{\\sigma^2}{2 \\theta} \\implies \\text{Var}(X) = \\frac{\\sigma^2}{2 \\theta} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JvTPsTTSMp4"
      },
      "source": [
        "---\n",
        "<font color=green>Q22: (2 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Create a function named `extract_s_scores` which computes 's scores' for the last element in a list of floating-point numbers. This function calculates the scores using the following formula $ \\text{s scores} = \\frac{X_T - \\mu}{\\sigma} $ where `list_xi` represents a list containing a sequence of floating-point numbers $(X_0, \\cdots, X_T)$.\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "unNHgTj8xkv5"
      },
      "outputs": [],
      "source": [
        "def extract_s_scores(list_xi):\n",
        "    mu = np.mean(list_xi)\n",
        "    x = list_xi[len(list_xi)-1]\n",
        "    sigma = np.std(list_xi)\n",
        "    Z = (x-mu)/sigma\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.414213562373095\n"
          ]
        }
      ],
      "source": [
        "#test the function\n",
        "list_xi = [1,2,3,4,5]\n",
        "s_score = extract_s_scores(list_xi)\n",
        "print(s_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAB_ANF2gCiY"
      },
      "source": [
        "# Autoencoder Analysis\n",
        "\n",
        "Autoencoders are neural networks used for unsupervised learning, particularly for dimensionality reduction and feature extraction. Training an autoencoder on the $Z_i$ matrix aims to identify hidden factors capturing the intrinsic structures in financial data.\n",
        "\n",
        "### Architecture\n",
        "- **Encoder**: Compresses input data into a smaller latent space representation.\n",
        "  - *Input Layer*: Matches the number of features in the $Z_i$ matrix.\n",
        "  - *Hidden Layers*: Compress data through progressively smaller layers.\n",
        "  - *Latent Space*: Encodes the data into hidden factors.\n",
        "- **Decoder**: Reconstructs input data from the latent space.\n",
        "  - *Hidden Layers*: Gradually expand to the original dimension.\n",
        "  - *Output Layer*: Matches the input layer to recreate the original matrix.\n",
        "\n",
        "### Training\n",
        "The autoencoder is trained by minimizing reconstruction loss, usually mean squared error (MSE), between the input $Z_i$ matrix and the decoder's output.\n",
        "\n",
        "### Hidden Factors Extraction\n",
        "After training, the encoder's latent space provides the most important underlying patterns in the stock returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJzNtknXgdqF"
      },
      "source": [
        "---\n",
        "<font color=green>Q23: (2 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Modify the standardized returns matrix `Z_matrix` to reduce the influence of extreme outliers on model trainingby ensuring that all values in the matrix `Z_matrix` do not exceed 3 standard deviations from the mean. Specifically, cap these values at the interval $-3, 3]$. Store the adjusted values in a new matrix, `Z_hat`.\n",
        "</font>\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Ln7vWV0TxmFk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>ADI</th>\n",
              "      <th>ANSS</th>\n",
              "      <th>...</th>\n",
              "      <th>TTWO</th>\n",
              "      <th>TMUS</th>\n",
              "      <th>TSLA</th>\n",
              "      <th>TXN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>WBA</th>\n",
              "      <th>WBD</th>\n",
              "      <th>WDAY</th>\n",
              "      <th>XEL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>0.658925</td>\n",
              "      <td>2.189521</td>\n",
              "      <td>0.106285</td>\n",
              "      <td>0.207821</td>\n",
              "      <td>1.503570</td>\n",
              "      <td>0.444433</td>\n",
              "      <td>1.020467</td>\n",
              "      <td>0.727398</td>\n",
              "      <td>1.860839</td>\n",
              "      <td>0.356414</td>\n",
              "      <td>...</td>\n",
              "      <td>0.517876</td>\n",
              "      <td>0.615242</td>\n",
              "      <td>0.814986</td>\n",
              "      <td>1.344650</td>\n",
              "      <td>1.105776</td>\n",
              "      <td>0.127319</td>\n",
              "      <td>0.497115</td>\n",
              "      <td>0.429736</td>\n",
              "      <td>2.277783</td>\n",
              "      <td>1.269711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>0.273051</td>\n",
              "      <td>-0.221306</td>\n",
              "      <td>-0.389329</td>\n",
              "      <td>-0.434766</td>\n",
              "      <td>0.787034</td>\n",
              "      <td>0.526996</td>\n",
              "      <td>0.277291</td>\n",
              "      <td>0.076716</td>\n",
              "      <td>1.630406</td>\n",
              "      <td>0.936497</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109884</td>\n",
              "      <td>0.439972</td>\n",
              "      <td>0.233493</td>\n",
              "      <td>1.187056</td>\n",
              "      <td>0.239468</td>\n",
              "      <td>-0.525980</td>\n",
              "      <td>0.691243</td>\n",
              "      <td>0.446100</td>\n",
              "      <td>0.389915</td>\n",
              "      <td>0.430715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>0.360570</td>\n",
              "      <td>1.136308</td>\n",
              "      <td>1.540732</td>\n",
              "      <td>1.439604</td>\n",
              "      <td>0.531735</td>\n",
              "      <td>-0.056439</td>\n",
              "      <td>0.475365</td>\n",
              "      <td>0.008639</td>\n",
              "      <td>0.935425</td>\n",
              "      <td>1.044069</td>\n",
              "      <td>...</td>\n",
              "      <td>1.337544</td>\n",
              "      <td>0.117646</td>\n",
              "      <td>2.058867</td>\n",
              "      <td>0.640237</td>\n",
              "      <td>0.325258</td>\n",
              "      <td>0.538943</td>\n",
              "      <td>-0.008816</td>\n",
              "      <td>0.565842</td>\n",
              "      <td>1.611597</td>\n",
              "      <td>0.597545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-03</th>\n",
              "      <td>-0.713053</td>\n",
              "      <td>-2.259591</td>\n",
              "      <td>0.252735</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>-0.591892</td>\n",
              "      <td>-0.600163</td>\n",
              "      <td>-0.086824</td>\n",
              "      <td>0.759731</td>\n",
              "      <td>-0.328378</td>\n",
              "      <td>-0.555499</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.377687</td>\n",
              "      <td>1.191774</td>\n",
              "      <td>-2.030038</td>\n",
              "      <td>-0.684867</td>\n",
              "      <td>-0.214460</td>\n",
              "      <td>0.183344</td>\n",
              "      <td>1.205798</td>\n",
              "      <td>-0.547975</td>\n",
              "      <td>-0.634285</td>\n",
              "      <td>0.121519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-04</th>\n",
              "      <td>0.560730</td>\n",
              "      <td>-1.137430</td>\n",
              "      <td>0.099652</td>\n",
              "      <td>0.013877</td>\n",
              "      <td>0.658632</td>\n",
              "      <td>-0.342217</td>\n",
              "      <td>0.223111</td>\n",
              "      <td>0.872403</td>\n",
              "      <td>-0.399698</td>\n",
              "      <td>-0.127907</td>\n",
              "      <td>...</td>\n",
              "      <td>1.434972</td>\n",
              "      <td>-0.347689</td>\n",
              "      <td>-0.377655</td>\n",
              "      <td>-1.394525</td>\n",
              "      <td>-0.538698</td>\n",
              "      <td>-0.487155</td>\n",
              "      <td>0.553153</td>\n",
              "      <td>0.755053</td>\n",
              "      <td>-0.537767</td>\n",
              "      <td>1.011197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-22</th>\n",
              "      <td>-1.146803</td>\n",
              "      <td>-0.516681</td>\n",
              "      <td>1.151431</td>\n",
              "      <td>1.085070</td>\n",
              "      <td>0.074915</td>\n",
              "      <td>0.081847</td>\n",
              "      <td>-0.150701</td>\n",
              "      <td>-0.278013</td>\n",
              "      <td>-0.555224</td>\n",
              "      <td>0.126867</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046870</td>\n",
              "      <td>-0.246035</td>\n",
              "      <td>-0.386620</td>\n",
              "      <td>-0.054031</td>\n",
              "      <td>-0.476030</td>\n",
              "      <td>-0.091837</td>\n",
              "      <td>-0.421327</td>\n",
              "      <td>-0.946797</td>\n",
              "      <td>0.106149</td>\n",
              "      <td>-0.002851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-25</th>\n",
              "      <td>0.659349</td>\n",
              "      <td>-1.221008</td>\n",
              "      <td>-0.372489</td>\n",
              "      <td>-0.341074</td>\n",
              "      <td>0.109668</td>\n",
              "      <td>-0.292706</td>\n",
              "      <td>-0.084892</td>\n",
              "      <td>1.184710</td>\n",
              "      <td>-0.959285</td>\n",
              "      <td>-0.282029</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.575803</td>\n",
              "      <td>0.240999</td>\n",
              "      <td>0.343241</td>\n",
              "      <td>-0.651297</td>\n",
              "      <td>-1.151639</td>\n",
              "      <td>-0.024341</td>\n",
              "      <td>0.166127</td>\n",
              "      <td>0.118796</td>\n",
              "      <td>-0.427841</td>\n",
              "      <td>0.321648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-26</th>\n",
              "      <td>-0.032708</td>\n",
              "      <td>0.234343</td>\n",
              "      <td>0.131651</td>\n",
              "      <td>0.109342</td>\n",
              "      <td>-0.556130</td>\n",
              "      <td>-0.244717</td>\n",
              "      <td>-0.377797</td>\n",
              "      <td>0.183364</td>\n",
              "      <td>-0.577464</td>\n",
              "      <td>0.317533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150958</td>\n",
              "      <td>-0.070195</td>\n",
              "      <td>0.960796</td>\n",
              "      <td>-1.177043</td>\n",
              "      <td>-0.384593</td>\n",
              "      <td>0.306386</td>\n",
              "      <td>-0.206326</td>\n",
              "      <td>-0.246683</td>\n",
              "      <td>0.237594</td>\n",
              "      <td>-0.878121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-27</th>\n",
              "      <td>-0.363721</td>\n",
              "      <td>1.052056</td>\n",
              "      <td>-0.024166</td>\n",
              "      <td>-0.010591</td>\n",
              "      <td>0.315892</td>\n",
              "      <td>0.224881</td>\n",
              "      <td>2.192442</td>\n",
              "      <td>1.128111</td>\n",
              "      <td>1.411127</td>\n",
              "      <td>-0.309662</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034693</td>\n",
              "      <td>0.474269</td>\n",
              "      <td>0.396878</td>\n",
              "      <td>1.991081</td>\n",
              "      <td>0.941800</td>\n",
              "      <td>-0.265792</td>\n",
              "      <td>1.179504</td>\n",
              "      <td>1.004419</td>\n",
              "      <td>-0.793726</td>\n",
              "      <td>2.193537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-28</th>\n",
              "      <td>-0.048375</td>\n",
              "      <td>0.411934</td>\n",
              "      <td>-0.078409</td>\n",
              "      <td>0.019962</td>\n",
              "      <td>0.022729</td>\n",
              "      <td>0.067776</td>\n",
              "      <td>1.190631</td>\n",
              "      <td>-0.583106</td>\n",
              "      <td>1.407557</td>\n",
              "      <td>-0.141870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.577964</td>\n",
              "      <td>0.645940</td>\n",
              "      <td>-0.749067</td>\n",
              "      <td>0.514492</td>\n",
              "      <td>0.585964</td>\n",
              "      <td>0.026648</td>\n",
              "      <td>1.496052</td>\n",
              "      <td>0.367481</td>\n",
              "      <td>-0.251144</td>\n",
              "      <td>0.527595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>252 rows × 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
              "Date                                                                     \n",
              "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
              "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
              "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
              "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
              "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
              "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
              "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
              "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
              "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
              "\n",
              "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
              "Date                                                ...                       \n",
              "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
              "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
              "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
              "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
              "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
              "...              ...       ...       ...       ...  ...       ...       ...   \n",
              "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
              "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
              "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
              "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
              "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
              "\n",
              "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
              "Date                                                                     \n",
              "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
              "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
              "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
              "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
              "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
              "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
              "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
              "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
              "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
              "\n",
              "                WDAY       XEL  \n",
              "Date                            \n",
              "2023-03-29  2.277783  1.269711  \n",
              "2023-03-30  0.389915  0.430715  \n",
              "2023-03-31  1.611597  0.597545  \n",
              "2023-04-03 -0.634285  0.121519  \n",
              "2023-04-04 -0.537767  1.011197  \n",
              "...              ...       ...  \n",
              "2024-03-22  0.106149 -0.002851  \n",
              "2024-03-25 -0.427841  0.321648  \n",
              "2024-03-26  0.237594 -0.878121  \n",
              "2024-03-27 -0.793726  2.193537  \n",
              "2024-03-28 -0.251144  0.527595  \n",
              "\n",
              "[252 rows x 89 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "Z_hat = np.clip(Z_matrix, -3, 3)\n",
        "Z_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNJDLYZ9g_V4"
      },
      "source": [
        "---\n",
        "<font color=green>Q24: (1 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Fetch the `Z_hat` data from GitHub, and we'll proceed with it now.\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BraH_nivxngd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>ADI</th>\n",
              "      <th>...</th>\n",
              "      <th>TTWO</th>\n",
              "      <th>TMUS</th>\n",
              "      <th>TSLA</th>\n",
              "      <th>TXN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>WBA</th>\n",
              "      <th>WBD</th>\n",
              "      <th>WDAY</th>\n",
              "      <th>XEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-03-29</td>\n",
              "      <td>0.658925</td>\n",
              "      <td>2.189521</td>\n",
              "      <td>0.106285</td>\n",
              "      <td>0.207821</td>\n",
              "      <td>1.503570</td>\n",
              "      <td>0.444433</td>\n",
              "      <td>1.020467</td>\n",
              "      <td>0.727398</td>\n",
              "      <td>1.860839</td>\n",
              "      <td>...</td>\n",
              "      <td>0.517876</td>\n",
              "      <td>0.615242</td>\n",
              "      <td>0.814986</td>\n",
              "      <td>1.344650</td>\n",
              "      <td>1.105776</td>\n",
              "      <td>0.127319</td>\n",
              "      <td>0.497115</td>\n",
              "      <td>0.429736</td>\n",
              "      <td>2.277783</td>\n",
              "      <td>1.269711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-03-30</td>\n",
              "      <td>0.273051</td>\n",
              "      <td>-0.221306</td>\n",
              "      <td>-0.389329</td>\n",
              "      <td>-0.434766</td>\n",
              "      <td>0.787034</td>\n",
              "      <td>0.526996</td>\n",
              "      <td>0.277291</td>\n",
              "      <td>0.076716</td>\n",
              "      <td>1.630406</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109884</td>\n",
              "      <td>0.439972</td>\n",
              "      <td>0.233493</td>\n",
              "      <td>1.187056</td>\n",
              "      <td>0.239468</td>\n",
              "      <td>-0.525980</td>\n",
              "      <td>0.691243</td>\n",
              "      <td>0.446100</td>\n",
              "      <td>0.389915</td>\n",
              "      <td>0.430715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>0.360570</td>\n",
              "      <td>1.136308</td>\n",
              "      <td>1.540732</td>\n",
              "      <td>1.439604</td>\n",
              "      <td>0.531735</td>\n",
              "      <td>-0.056439</td>\n",
              "      <td>0.475365</td>\n",
              "      <td>0.008639</td>\n",
              "      <td>0.935425</td>\n",
              "      <td>...</td>\n",
              "      <td>1.337544</td>\n",
              "      <td>0.117646</td>\n",
              "      <td>2.058867</td>\n",
              "      <td>0.640237</td>\n",
              "      <td>0.325258</td>\n",
              "      <td>0.538943</td>\n",
              "      <td>-0.008816</td>\n",
              "      <td>0.565842</td>\n",
              "      <td>1.611597</td>\n",
              "      <td>0.597545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-04-03</td>\n",
              "      <td>-0.713053</td>\n",
              "      <td>-2.259591</td>\n",
              "      <td>0.252735</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>-0.591892</td>\n",
              "      <td>-0.600163</td>\n",
              "      <td>-0.086824</td>\n",
              "      <td>0.759731</td>\n",
              "      <td>-0.328378</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.377687</td>\n",
              "      <td>1.191774</td>\n",
              "      <td>-2.030038</td>\n",
              "      <td>-0.684867</td>\n",
              "      <td>-0.214460</td>\n",
              "      <td>0.183344</td>\n",
              "      <td>1.205798</td>\n",
              "      <td>-0.547975</td>\n",
              "      <td>-0.634285</td>\n",
              "      <td>0.121519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-04-04</td>\n",
              "      <td>0.560730</td>\n",
              "      <td>-1.137430</td>\n",
              "      <td>0.099652</td>\n",
              "      <td>0.013877</td>\n",
              "      <td>0.658632</td>\n",
              "      <td>-0.342217</td>\n",
              "      <td>0.223111</td>\n",
              "      <td>0.872403</td>\n",
              "      <td>-0.399698</td>\n",
              "      <td>...</td>\n",
              "      <td>1.434972</td>\n",
              "      <td>-0.347689</td>\n",
              "      <td>-0.377655</td>\n",
              "      <td>-1.394525</td>\n",
              "      <td>-0.538698</td>\n",
              "      <td>-0.487155</td>\n",
              "      <td>0.553153</td>\n",
              "      <td>0.755053</td>\n",
              "      <td>-0.537767</td>\n",
              "      <td>1.011197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>2024-03-22</td>\n",
              "      <td>-1.146803</td>\n",
              "      <td>-0.516681</td>\n",
              "      <td>1.151431</td>\n",
              "      <td>1.085070</td>\n",
              "      <td>0.074915</td>\n",
              "      <td>0.081847</td>\n",
              "      <td>-0.150701</td>\n",
              "      <td>-0.278013</td>\n",
              "      <td>-0.555224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046870</td>\n",
              "      <td>-0.246035</td>\n",
              "      <td>-0.386620</td>\n",
              "      <td>-0.054031</td>\n",
              "      <td>-0.476030</td>\n",
              "      <td>-0.091837</td>\n",
              "      <td>-0.421327</td>\n",
              "      <td>-0.946797</td>\n",
              "      <td>0.106149</td>\n",
              "      <td>-0.002851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>2024-03-25</td>\n",
              "      <td>0.659349</td>\n",
              "      <td>-1.221008</td>\n",
              "      <td>-0.372489</td>\n",
              "      <td>-0.341074</td>\n",
              "      <td>0.109668</td>\n",
              "      <td>-0.292706</td>\n",
              "      <td>-0.084892</td>\n",
              "      <td>1.184710</td>\n",
              "      <td>-0.959285</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.575803</td>\n",
              "      <td>0.240999</td>\n",
              "      <td>0.343241</td>\n",
              "      <td>-0.651297</td>\n",
              "      <td>-1.151639</td>\n",
              "      <td>-0.024341</td>\n",
              "      <td>0.166127</td>\n",
              "      <td>0.118796</td>\n",
              "      <td>-0.427841</td>\n",
              "      <td>0.321648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>2024-03-26</td>\n",
              "      <td>-0.032708</td>\n",
              "      <td>0.234343</td>\n",
              "      <td>0.131651</td>\n",
              "      <td>0.109342</td>\n",
              "      <td>-0.556130</td>\n",
              "      <td>-0.244717</td>\n",
              "      <td>-0.377797</td>\n",
              "      <td>0.183364</td>\n",
              "      <td>-0.577464</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150958</td>\n",
              "      <td>-0.070195</td>\n",
              "      <td>0.960796</td>\n",
              "      <td>-1.177043</td>\n",
              "      <td>-0.384593</td>\n",
              "      <td>0.306386</td>\n",
              "      <td>-0.206326</td>\n",
              "      <td>-0.246683</td>\n",
              "      <td>0.237594</td>\n",
              "      <td>-0.878121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>2024-03-27</td>\n",
              "      <td>-0.363721</td>\n",
              "      <td>1.052056</td>\n",
              "      <td>-0.024166</td>\n",
              "      <td>-0.010591</td>\n",
              "      <td>0.315892</td>\n",
              "      <td>0.224881</td>\n",
              "      <td>2.192442</td>\n",
              "      <td>1.128111</td>\n",
              "      <td>1.411127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034693</td>\n",
              "      <td>0.474269</td>\n",
              "      <td>0.396878</td>\n",
              "      <td>1.991081</td>\n",
              "      <td>0.941800</td>\n",
              "      <td>-0.265792</td>\n",
              "      <td>1.179504</td>\n",
              "      <td>1.004419</td>\n",
              "      <td>-0.793726</td>\n",
              "      <td>2.193537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>2024-03-28</td>\n",
              "      <td>-0.048375</td>\n",
              "      <td>0.411934</td>\n",
              "      <td>-0.078409</td>\n",
              "      <td>0.019962</td>\n",
              "      <td>0.022729</td>\n",
              "      <td>0.067776</td>\n",
              "      <td>1.190631</td>\n",
              "      <td>-0.583106</td>\n",
              "      <td>1.407557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.577964</td>\n",
              "      <td>0.645940</td>\n",
              "      <td>-0.749067</td>\n",
              "      <td>0.514492</td>\n",
              "      <td>0.585964</td>\n",
              "      <td>0.026648</td>\n",
              "      <td>1.496052</td>\n",
              "      <td>0.367481</td>\n",
              "      <td>-0.251144</td>\n",
              "      <td>0.527595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>252 rows × 90 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date      ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
              "0    2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
              "1    2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
              "2    2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
              "3    2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
              "4    2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
              "..          ...       ...       ...       ...       ...       ...       ...   \n",
              "247  2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
              "248  2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
              "249  2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
              "250  2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
              "251  2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
              "\n",
              "          AEP      AMGN       ADI  ...      TTWO      TMUS      TSLA  \\\n",
              "0    1.020467  0.727398  1.860839  ...  0.517876  0.615242  0.814986   \n",
              "1    0.277291  0.076716  1.630406  ... -0.109884  0.439972  0.233493   \n",
              "2    0.475365  0.008639  0.935425  ...  1.337544  0.117646  2.058867   \n",
              "3   -0.086824  0.759731 -0.328378  ... -0.377687  1.191774 -2.030038   \n",
              "4    0.223111  0.872403 -0.399698  ...  1.434972 -0.347689 -0.377655   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "247 -0.150701 -0.278013 -0.555224  ...  0.046870 -0.246035 -0.386620   \n",
              "248 -0.084892  1.184710 -0.959285  ... -2.575803  0.240999  0.343241   \n",
              "249 -0.377797  0.183364 -0.577464  ...  0.150958 -0.070195  0.960796   \n",
              "250  2.192442  1.128111  1.411127  ...  0.034693  0.474269  0.396878   \n",
              "251  1.190631 -0.583106  1.407557  ...  0.577964  0.645940 -0.749067   \n",
              "\n",
              "          TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
              "0    1.344650  1.105776  0.127319  0.497115  0.429736  2.277783  1.269711  \n",
              "1    1.187056  0.239468 -0.525980  0.691243  0.446100  0.389915  0.430715  \n",
              "2    0.640237  0.325258  0.538943 -0.008816  0.565842  1.611597  0.597545  \n",
              "3   -0.684867 -0.214460  0.183344  1.205798 -0.547975 -0.634285  0.121519  \n",
              "4   -1.394525 -0.538698 -0.487155  0.553153  0.755053 -0.537767  1.011197  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "247 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797  0.106149 -0.002851  \n",
              "248 -0.651297 -1.151639 -0.024341  0.166127  0.118796 -0.427841  0.321648  \n",
              "249 -1.177043 -0.384593  0.306386 -0.206326 -0.246683  0.237594 -0.878121  \n",
              "250  1.991081  0.941800 -0.265792  1.179504  1.004419 -0.793726  2.193537  \n",
              "251  0.514492  0.585964  0.026648  1.496052  0.367481 -0.251144  0.527595  \n",
              "\n",
              "[252 rows x 90 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "Z_hat_path = r\"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_hat.csv\"\n",
        "Z_hat = pd.read_csv(Z_hat_path)\n",
        "Z_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3MRBtXIiWcN"
      },
      "source": [
        "---\n",
        "<font color=green>Q25: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Segment the standardized and capped returns matrix $\\hat{Z}$ into two subsets for model training and testing. Precisly Allocate 70% of the data in $\\hat{Z}$ to the training set $ \\hat{Z}_{train} $ and Allocate the remaining 30% to the testing set $\\hat{Z}_{test}$. Treat each stock within $\\hat{Z}$ as an individual sample, by flattening temporal dependencies.\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Shape of Matrix Z_hat is: (252, 90)\n",
            "The Split Index is: 176\n",
            "The Shape of Z_train is: (176, 89)\n",
            "The Shape of Z_test is: (76, 89)\n"
          ]
        }
      ],
      "source": [
        "\"\"\" \n",
        "\"Treat each stock within Z_hat as an individual sample, by flattening temporal dependencies\" \n",
        "means that instead of considering the time series data for each stock as dependent on previous time points, \n",
        "you treat each stock's entire time series as a single sample. In other words, you ignore the temporal order \n",
        "and dependencies between the time points and consider the data for each stock independently.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"The Shape of Matrix Z_hat is: {Z_hat.shape}\")\n",
        "\n",
        "# Calculate split index\n",
        "split_index = int(Z_hat.shape[0] * 0.7)\n",
        "\n",
        "print(f\"The Split Index is: {split_index}\")\n",
        "\n",
        "# Ensure Z_hat is of type float32\n",
        "Z_hat_numpy = Z_hat.values[:, 1:] # Remove the Date Column\n",
        "Z_hat_numpy = Z_hat_numpy.astype(np.float32) \n",
        "\n",
        "# Split into training and testing sets\n",
        "Z_train = Z_hat_numpy[:split_index, :]\n",
        "Z_test = Z_hat_numpy[split_index:, :]\n",
        "\n",
        "print(f\"The Shape of Z_train is: {Z_train.shape}\")\n",
        "print(f\"The Shape of Z_test is: {Z_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWeD_efihbH"
      },
      "source": [
        "---\n",
        "<font color=green>Q26: (10 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Please create an autoencoder following the instructions provided in  **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**, Use the model 'Variant 2' in Table 1.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Insert your code here\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hyf_dfsoxp_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Input Dimensions is set to the number of features: 89\n"
          ]
        }
      ],
      "source": [
        "# Define Architecture\n",
        "input_dim = Z_train.shape[1] # Number of Features\n",
        "print(f\"The Input Dimensions is set to the number of features: {input_dim}\")\n",
        "hidden_nodes = 20\n",
        "\n",
        "# Build Encoder\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "encoder = Dense(hidden_nodes, activation='tanh')(input_layer)\n",
        "\n",
        "# Build Decoder\n",
        "decoder = Dense(input_dim, activation='tanh')(encoder)\n",
        "\n",
        "# Build Autoencoder\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6o0QBPateR"
      },
      "source": [
        "---\n",
        "<font color=green>Q27 (1 Mark) :\n",
        "\n",
        "Display all the parameters of the deep neural network.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yLyuzLkGxrAd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,869</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m1,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m)             │         \u001b[38;5;34m1,869\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,669</span> (14.33 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,669\u001b[0m (14.33 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,669</span> (14.33 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,669\u001b[0m (14.33 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Insert your code here\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lNBOw3ait03"
      },
      "source": [
        "---\n",
        "<font color=green>Q28: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Train your model using the Adam optimizer for 20 epochs with a batch size equal to 8 and validation split to 20%. Specify the loss function you've chosen.\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IBUtUlxIxsrx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.9234 - val_loss: 0.9199\n",
            "Epoch 2/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8656 - val_loss: 0.8421\n",
            "Epoch 3/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7342 - val_loss: 0.7753\n",
            "Epoch 4/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7023 - val_loss: 0.7231\n",
            "Epoch 5/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6580 - val_loss: 0.6911\n",
            "Epoch 6/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6036 - val_loss: 0.6710\n",
            "Epoch 7/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5952 - val_loss: 0.6564\n",
            "Epoch 8/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5564 - val_loss: 0.6461\n",
            "Epoch 9/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5723 - val_loss: 0.6363\n",
            "Epoch 10/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5269 - val_loss: 0.6275\n",
            "Epoch 11/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5280 - val_loss: 0.6199\n",
            "Epoch 12/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5353 - val_loss: 0.6138\n",
            "Epoch 13/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5252 - val_loss: 0.6072\n",
            "Epoch 14/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4901 - val_loss: 0.6011\n",
            "Epoch 15/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5286 - val_loss: 0.5956\n",
            "Epoch 16/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4788 - val_loss: 0.5903\n",
            "Epoch 17/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4664 - val_loss: 0.5855\n",
            "Epoch 18/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5129 - val_loss: 0.5814\n",
            "Epoch 19/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4765 - val_loss: 0.5775\n",
            "Epoch 20/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4739 - val_loss: 0.5736\n"
          ]
        }
      ],
      "source": [
        "## Insert your code here\n",
        "# Specify Optimizer\n",
        "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the Model\n",
        "history = autoencoder.fit(Z_train, Z_train,\n",
        "                epochs = 20,\n",
        "                batch_size = 8, \n",
        "                validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "plt.plot(history.history['loss'], label = 'Training Loss')\n",
        "plt.plot(history.history['val_loss'], label = 'Testing Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xI8O24djAYO"
      },
      "source": [
        "---\n",
        "<font color=green>Q29: (3 Marks) </font>\n",
        "<br><font color='green'>\n",
        "Predict using the testing set and extract the residuals based on the methodology described in **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**.\n",
        "for 'NVDA' stock.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "S-EDTK7wxtxB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
          ]
        }
      ],
      "source": [
        "## Insert your code here\n",
        "\n",
        "# Create the Encoder Model to extract features\n",
        "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
        "\n",
        "# Extract hidden factors from the testing set\n",
        "hidden_factors_test = encoder_model.predict(Z_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the hidden factors matrix for testing set: (76, 20)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of the hidden factors matrix for testing set:\", hidden_factors_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "The Index Position for NVDA in the Z_train Array is: 62\n",
            "Residuals for NVDA stock:\n",
            "[ 0.36827883 -1.4619646   0.1886527  -0.15274066 -0.08035703 -0.1148288\n",
            "  0.24676743 -0.53119516 -0.6517728  -0.1651296  -0.1491823  -0.2684268\n",
            "  0.08265593  0.33282137  0.34644604 -0.23892653  0.08466017  0.4437214\n",
            "  0.4519459   1.2423688   0.18383825  0.6011215  -0.06320067  0.03597127\n",
            "  0.531245    0.16965762 -0.30700535  0.40768582  0.14412348  0.13483721\n",
            "  0.11003095 -0.08017263  0.19472855  0.17926991  0.7096476  -0.17022991\n",
            "  0.4699729   0.7535829   1.4997993  -0.15787947  0.17702055 -0.41494423\n",
            "  0.18506432  0.211601    0.5010381   0.00380498 -0.25186026  0.37135217\n",
            " -1.0163649  -0.774523    2.1518416   0.3881678  -0.01366841 -0.4517438\n",
            " -0.18993959 -0.22837034  0.41402447  0.8462615   0.7809165   0.89513326\n",
            "  0.58159786 -1.2405784  -0.1618663   1.5064461   0.29615062 -1.0738537\n",
            "  0.20898199  0.07793651  0.02964866 -0.51177084 -0.27746826  1.1741203\n",
            "  0.82995343 -0.5860585  -0.9960976   0.02057986]\n"
          ]
        }
      ],
      "source": [
        "# Predict using the testing set\n",
        "Z_test_pred = autoencoder.predict(Z_test)\n",
        "\n",
        "# Extract residuals\n",
        "residuals = Z_test - Z_test_pred\n",
        "\n",
        "NVDA_Index = Z_hat.columns.get_loc('NVDA') - 1 # Subtract 1 because we removed the date column\n",
        "print(f\"The Index Position for NVDA in the Z_train Array is: {NVDA_Index}\")\n",
        "\n",
        "# Replace index 0 with the actual index of NVDA in your dataset\n",
        "nvda_residuals = residuals[:, NVDA_Index]\n",
        "\n",
        "# Printing the residuals for NVDA\n",
        "print(f\"Residuals for NVDA stock:\")\n",
        "print(nvda_residuals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYyVOZu90CIK"
      },
      "source": [
        "<font color=green>Q30: (7 Marks) </font>\n",
        "<br><font color='green'>\n",
        "By reading carrefully the paper **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**, answers the following question:\n",
        "1. **Summarize the Key Actions**: Highlight the main experiments and methodologies employed by the authors in Section 5.\n",
        "2. **Reproduction Steps**: Detail the necessary steps required to replicate the authors' approach based on the descriptions provided in the paper.\n",
        "3. **Proposed Improvement**: Suggest one potential enhancement to the methodology that could potentially increase the effectiveness or efficiency of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na95zQB40hRG"
      },
      "source": [
        "**Write your answers here:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trading through statistical arbitrage models aims to capture stock return fluctuations. Such techniques are often prone to overfitting and data mining biases. In order to address these issues, an integrated autoencoder training technique is suggested. The model’s mechanics unifies estimations through an asset pricing model with the estimation of StatArb parameters allowing to make end-to-end decisions. Hence, it is important to explore the components of the system derived. \n",
        "The methodology employed included multiple steps.\n",
        "\n",
        "\n",
        "- Data:\n",
        "The first step consists of the data preparation. CRSP stock data was used running from January 2000 to December 2022 with a focus on US traded companies. The dataset was cleaned in such a way that primary listings were only included. Moreover, those were adjusted for stock splits and dividends and were restricted to stocks with the following features:\n",
        "* Closing price of above 5$\n",
        "* 20-day median market capitalization above $1 billion\n",
        "* 20-day median trading volume above $1 million.\n",
        "\n",
        "\n",
        "- Asset Pricing Models:\n",
        "The second step was to set the benchmark models. The two models that will be used are Fama French models and Principal component Analysis,\n",
        "For FFM, daily regressions were conducted using several factor models including CAPM, FF3, FF5, FF 5 MOM. Residuals were extracted as described earlier.\n",
        "For PCA, 1, 3, 5, 8, 10, 15, and 20 principal components were used to uncover latent factors. Afterwards, the correlation matrix was calculated for the past 252 trading days using standardised returns. \n",
        "\n",
        "\n",
        "- Model Training:\n",
        "For the next step, a training procedure was proposed for the autoencoder. Firstly, one needs an encoder with one layer of nodes l. Moreover, one will use the rectified linear unit as an activation function for the hidden layer. Simply, reLu will take the inputs given to the neural network and compute the activations of the neurons in the hidden layer. \n",
        "In addition, one needs a decoder. It also consists of a layer with similar dimensions to the input given. Under that scenario, the activation function that will be used is the hyperbolic tangent (tanh). \n",
        "The obtained values will be used to derive the loss (cost) function. It is intuitive to say that this loss function needs to be minimized.  The mean squared error loss function optimization is done using the Adam optimizer for 10 epochs. \n",
        "Different combinations of autoencoders were tested. In other words, one tried the model for different hyperparameters. This includes modifying the number of layers, changing activation functions, trying different biases (to allow only active neurons to be activated), dropout layers.\n",
        "Afterwards, the residuals computed (from the difference between the decoded ouputs and inputs) and the hyperbolic tangent activation function is applied to the policy layer to compute the needed weights for the assets of the portfolio. It is important to mention that standardised returns were capped at three standard deviations. This was done to prevent distortion from outliers.\n",
        "Residual generation\n",
        "\n",
        "In addition, different methods were used to generate residuals. This includes subtracting the autoencoder predicted standardised returns from the actual standardised returns. Moreover, latent layer factor returns were predicted using the model and a regression was ran to generate residuals. Finally, returns were scaled by volatility similarly to PCA. The backward looking standard deviation was used to scale those returns. A regression was ran using the scaled returns similarly to the latent factor.\n",
        "\n",
        "\n",
        "Signal Extraction Techniques\n",
        "Afterwards signals were extracted. According to an Ornstein-Uhlenbeck Process, the parameters were estimated. It is valuable to mention that under that scenario, a discretised mean reverting process was assumed. As a result, S-scores were generated for the signals. Moreover, an artificial neural network (3 layer feedforward network) was used to extract signals from the OU estimation output statistics. One should note that the network was trained on the Sharpe ratio.\n",
        "\n",
        "\n",
        "Backtesting and Performance Evaluation\n",
        "For the back testing, the OU parameters will be extracted with a 60-day lookback regression. The model was trained on 1000 days of OU parameters and the optimization will be done using the Sharpe ratio. The out-of-sample window will be taken to be 125 days for testing. The performance was evaluated on the US equity returns. The effectiveness of different architectures of the network was evaluated against the benchmark.\n",
        "Finally, the methodology could benefit from the integration of a transformer-based models. Transformers can be used to capture complex and long-term patterns in sequential data. It can be implemented to predict residuals in a time series. Transformers allows to give a weight to each time step leading to more accurate predictions. This can enhance the profitability of each trading signal generated from the residuals. In addition, transformers are appropriate to be used in large-scale data. In other words, they can handle large datasets which is a common scenario in finance. Moreover, it is important to mention that the risk of overfitting would be reduced realizing a better generalization across diverse data patterns. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
